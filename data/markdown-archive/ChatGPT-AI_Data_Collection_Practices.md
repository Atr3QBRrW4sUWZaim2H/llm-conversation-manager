---
title: AI Data Collection Practices
source: https://chatgpt.com/c/67aec176-06c4-8003-8800-45dff49503c7
---

# AI Data Collection Practices

#### You:
<time datetime="2025-02-14T05:33:23.712Z" title="2/14/2025, 2:33:23 PM">14:33</time>

Industry Practices in AI and Personal Data (2022–2024)
In the past 1–2 years, leading AI companies have faced increasing scrutiny over how they handle personal data. Tech firms like OpenAI, Google, Meta, Amazon, and Microsoft have begun disclosing what user data their AI systems collect, how long they retain it, and what controls users have. Below, we examine recent practices around data collection, retention, and user control, focusing on data types critical to user privacy (such as browsing history, chat/voice inputs, and behavioral data) and noting the shift toward on-device processing for privacy.
Data Collection and Storage by Leading AI Companies
OpenAI (ChatGPT): OpenAI’s consumer AI services (e.g. ChatGPT) collect the text prompts and conversations users input, and may log other usage details (like timestamps and account info). By default, ChatGPT conversations are saved to a user’s account and also used to improve OpenAI’s models unless the user opts out (Stop OpenAI training its models on your chats by turning off history) (Italy lifts ban on ChatGPT after data privacy improvements - DW). OpenAI stores conversation data on its servers, but in 2023 it introduced Data Controls that let users turn off chat history – which stops using those chats for training models (Stop OpenAI training its models on your chats by turning off history). Even with history off, OpenAI retains conversation records for 30 days on its servers (for misuse monitoring) before deleting them (Stop OpenAI training its models on your chats by turning off history). For its API services and enterprise products, OpenAI has stricter policies: data sent via the API is not used to train or improve models by default (since March 1, 2023) (Does the openai API get access to the data I send it or store the data), and is generally stored no longer than 30 days for abuse detection (Enterprise privacy at OpenAI). OpenAI also collects some user metadata (account details, possibly IP address/location) per its privacy policy (Garante issues notice to OpenAI over alleged GDPR violations | IAPP), but personal data usage is minimized for API/enterprise customers.

Google (Bard and Assistant): Google’s AI offerings such as Bard (the chat AI) and Google Assistant collect a wide range of personal data, integrated with the user’s Google account. Bard logs the prompts you enter and its responses, associating them with your Google account’s “Bard Activity.” By default, Google stores Bard conversation history for 18 months (the default auto-delete period for Web & App Activity) and allows users to adjust the retention period between 3 months and 36 months (How to Use Google Bard AI: A Step-by-Step Guide - GoZen.io). Users can manually delete Bard conversations from their account history, or turn off Bard Activity tracking entirely, which stops saving future chats to your account. However, even with Bard Activity turned off, Google still keeps recent conversations for 72 hours on their servers (for “safety and reliability” purposes) before deletion (Teardown reveals Assistant with Bard is moving closer to Android ...). Google has stated that Bard conversations may be reviewed by human moderators to improve quality, and such reviewed chats might not be immediately deleted even if you clear your history (Google's new Gemini-powered Bard raises privacy eyebrows). Besides Bard, Google Assistant voice interactions are also stored in the user’s account history (audio recordings and transcripts). Google gives users control to review and delete Assistant voice recordings in the My Activity portal, and offers settings like auto-deletion after a set time or not saving audio at all (Where are we on privacy for pixel phones? : r/GooglePixel - Reddit). In terms of breadth, Google collects extensive behavioral data (search queries, YouTube activity, location if enabled, etc.) which feeds into AI-driven services (like personalized recommendations), though not all of this is used for training generative AI. Notably, Google’s privacy policy assures that Bard data isn’t used for ad targeting (ChatGPT Or Google Bard? Privacy Or Performance? All Questions ...), keeping it separate from advertising datasets. All stored data resides on Google’s cloud servers (distributed globally), with Google employing encryption and strict access controls on personal data.

Meta (Facebook/Instagram AI Features): Meta has increasingly integrated AI into its platforms and has signaled plans to leverage the massive personal data it holds for AI model training. Meta’s social platforms collect virtually all user-provided content and interactions – posts, messages, photos, likes, friend networks, browsing of third-party sites via Facebook Pixel, etc. Traditionally this data powers personalized feeds and ads via machine learning. In 2023–2024 Meta began rolling out generative AI features (like AI chatbots on Messenger/Instagram and image generation tools). According to Meta’s disclosures, the company intends to use public user content to train its AI models (for example, using Facebook posts and Instagram photos to refine AI systems) (Meta to Use Public User Data for AI Training, Allows EU Opt Out). This raised major privacy concerns, especially in the EU. In mid-2024, Meta announced updates to its privacy policy to clarify this use: essentially all public posts and data may be used for AI development, and users outside the EU have no way to opt out, while European users were given a limited right to object (Meta to Use Public User Data for AI Training, Allows EU Opt Out). (Meta confirmed it does not use sensitive data like personal messages or EU users’ data in training at this time (Why Opting Out of Meta's Use of Facebook, Instagram Posts for AI ...).) All user data is stored on Meta’s servers (primarily in the cloud), and Meta’s retention policies generally keep data as long as your account is active. Users can delete or deactivate their accounts to stop future data collection, but any data already used to train models or build profiles may persist in aggregated form. Meta faced regulatory pushback in Europe, delaying the launch of some AI features until it could address privacy requirements (Meta pauses AI models launch in Europe due to Irish request | Reuters). The company has started providing data export and objection forms for users to exercise rights over their data used in AI, at least in jurisdictions where required. However, practical user control at Meta remains limited – once you’ve shared content, Meta’s terms permit using it for various purposes, including machine learning, unless law compels an opt-out.

Amazon (Alexa and Services): Amazon’s AI primarily appears in consumer devices (the Alexa voice assistant on Echo speakers, and AI features on Amazon’s shopping platform). Alexa voice interactions involve recording your voice commands and sending them to Amazon’s cloud for processing. Amazon historically stored Alexa voice recordings indefinitely by default, along with transcripts of what was said (Amazon & FTC Agree $25 Million Settlement to Resolve Alleged ...). In recent years, Amazon introduced user controls to delete Alexa voice history – either manually via the Alexa app or automatically (users can set recordings to auto-delete after 3 or 18 months, or choose not to save recordings at all) (Delete Alexa Voice Recordings Automatically - Amazon.com). Despite these controls, Amazon ran into trouble for failing to fully honor deletions: a 2023 FTC investigation found Amazon kept children’s Alexa recordings and retained voice transcripts even after audio was deleted, violating privacy laws (FTC Announces Enforcement Actions Against Amazon's Alexa and ...). Amazon agreed to fix these issues by deleting data when requested and paying fines. Aside from voice commands, Alexa devices also collect smart home data (e.g. device usage, queries about your calendar or shopping) that Amazon may use to improve services. Human reviewers have in the past listened to a sample of Alexa recordings to train and refine the AI’s understanding (Is Alexa always listening? How to protect your data from Amazon), though Amazon now allows users to opt out of their recordings being used for Alexa development (Is Alexa always listening? How to protect your data from Amazon). Beyond Alexa, Amazon’s retail platform tracks user browsing and purchase history extensively for its recommendation algorithms. That behavioral data (what you view, click, and buy) is stored in Amazon’s systems and fuels AI-driven product recommendations and targeted ads. Studies in 2022 showed Amazon even leverages Alexa voice data for advertising – researchers found Echo smart speakers were collecting voice interaction data and sharing it with advertising partners to target ads without clear consent (Internet Privacy Laws Fall Short of Protecting People. A New ...). All of this data is stored on Amazon servers. Users have some controls, like clearing their Amazon browsing history or opting out of interest-based ads on Amazon, but little ability to prevent Amazon from using their Alexa interactions or shopping activity in aggregate to improve its AI models aside from not using the services.

Microsoft (Bing, Windows Copilot, etc.): Microsoft collects personal data across its many services, and in the AI realm this includes Bing search and chat queries, GitHub Copilot/Copilot X interactions, and Microsoft 365 Copilot usage. When you use Bing Chat (the AI chatbot integrated with Bing search), your conversation (queries and the AI’s replies) is sent to Microsoft’s servers and likely logged similarly to a search query. Microsoft uses these logs to monitor for abuse and improve answer quality. As of 2023, there isn’t a user-facing tool to download or delete Bing Chat history (though you can clear it from the interface), and Microsoft’s privacy policy doesn’t detail the retention period for Bing chats. However, Microsoft has made strong privacy pledges for its enterprise AI services. For instance, Microsoft 365 Copilot (which can analyze your private Office documents and emails with GPT-like intelligence) is designed with privacy in mind: your data stays within your tenant and “isn’t used to train the foundation LLMs” or any models outside your organization (Data, Privacy, and Security for Microsoft 365 Copilot). In other words, Microsoft promises that content from your private documents or Copilot prompts will not feed back into the AI’s learning process that serves other customers. Similarly, the Azure OpenAI Service (Microsoft’s cloud offering of OpenAI models) does not use customer-provided data to train models (Azure OpenAI Service frequently asked questions - Microsoft Learn), aligning with OpenAI’s API policy. Microsoft stores enterprise AI data securely and deletes it according to contract policies. For consumer-side AI like Bing and Windows Copilot for Windows 11, Microsoft likely applies standard telemetry practices – data is collected to improve the product but not used to retrain base models. Microsoft also provides general privacy dashboards where users can see and delete data like search history, Cortana voice queries (for the now-deprecated Cortana assistant), and Windows diagnostic data. Overall, Microsoft’s recent approach separates customer-specific data from foundation model training, focusing on gaining enterprise trust (Copliot Privacy Concerns ++ - Microsoft Community). Personal data is retained for only as long as needed for the specific service (with many Bing and Microsoft account data types defaulting to 18 months or less retention, unless a user deletes them sooner).
Data Types AI Models Retain and Use
Modern AI systems can ingest various personal data types. Key categories and how companies handle them include:

Chat and Text Input: All the chatbots (ChatGPT, Bard, Meta AI assistants, Bing Chat) retain the text you input, at least temporarily. By default this text might be used to fine-tune or improve the model (as OpenAI and Google do), unless you opt out. For instance, OpenAI uses ChatGPT prompts to refine future responses unless you disable that in settings (Stop OpenAI training its models on your chats by turning off history). Google’s Bard similarly uses conversation logs to improve the model’s quality, with human reviewers possibly reading some chats (Google's new Gemini-powered Bard raises privacy eyebrows). Microsoft claims not to reuse private Copilot prompts for training (Data, Privacy, and Security for Microsoft 365 Copilot), and OpenAI doesn’t train on API-submitted text (Does the openai API get access to the data I send it or store the data). User control: Typically you can delete past chats (removing them from your history view), but the company may still hold them for a period in backend systems. Opt-out toggles (ChatGPT’s “do not train on my content” or Bard’s activity off) give partial control over this data.

Browsing History and Behavioral Tracking: This refers to your web activity, app usage, and other behavioral signals. Google and Meta are most active here – Google logs your searches, YouTube watches, and Chrome browsing (if synced) to personalize services with AI. Meta tracks your Facebook/Instagram usage and even your off-platform browsing via plugins, feeding that into its algorithms. These data are used for recommendation AIs (e.g. news feed ranking, ad targeting) more than for generative AI training. User control: You can pause or clear Google’s Web & App Activity, and Facebook offers an “Off-Facebook Activity” tool to disconnect third-party browsing data from your profile. Such measures don’t completely stop tracking but give users some say in what’s stored. In general, browsing history is not directly fed into large language model training without anonymization, but it strongly influences the AI-driven personalization you see. For example, Amazon and Google use behavioral profiles to recommend products or results, and these profiles are built by AI models analyzing your past behavior. Users can delete their Amazon browsing history or YouTube watch history to reset those algorithms, albeit with effort.

Voice and Audio Input: Voice assistants like Amazon Alexa, Google Assistant, Apple Siri, and (previously) Microsoft Cortana all handle audio recordings of your queries. These recordings are usually transcribed to text and stored. Amazon Alexa keeps audio and text logs of every command; Google Assistant does similarly for “Hey Google” queries. Companies use this data to train speech recognition and natural language understanding models – often employing human auditors to correct transcripts and improve AI. In recent years, backlash over privacy led to more transparency. Amazon: As noted, it now allows opting out of Alexa voice clip review, but still uses many recordings to improve Alexa by default (Is Alexa always listening? How to protect your data from Amazon). Google: On Pixel phones, Google began doing on-device voice processing for Assistant (starting around Pixel 7) so that your voice commands can be understood without always sending audio to the cloud (Google Assistant Uses On-Device AI for Pixel 7 and 7 ... - Voicebot.ai). This limits what data leaves the device. For cloud-processed voice, Google provides a Voice & Audio Activity toggle – if turned off, Google says it won’t retain your audio recordings (only live transcripts for processing). Microsoft: Cortana data (when it was active) could be deleted via the privacy dashboard. User control: Both Amazon and Google enable users to review and delete voice recordings. Amazon even lets you say “Alexa, delete what I just said” to remove the latest recording. Still, as the FTC findings show, deletion may not always scrub all derived data (e.g., transcripts) unless policies are enforced (FTC Announces Enforcement Actions Against Amazon's Alexa and ...). Voice data is sensitive, so privacy advocates urge regularly purging these histories or disabling retention if possible.

Personal Files and Communications: New AI features that use your private files (emails, documents, photos) raise questions about data use. Microsoft 365 Copilot and Google’s suggested “Help me write” in Gmail both process user content. These systems typically process data on the fly and do not add your private content into general AI training. Microsoft explicitly promises your documents stay within your tenant (Data, Privacy, and Security for Microsoft 365 Copilot). Google’s workspace AI features likely follow a similar approach (your email to draft a reply is not used to train Gmail’s model for everyone). However, if you use cloud services, the content is stored in those clouds and subject to general retention and subpoena policies. User control: Apart from deleting the content from the service (e.g. deleting a file or email), users rely on company policy here. On personal devices, some AI (like photo apps categorizing your pictures) can work on-device, meaning the data stays local.

Derived Data and Profiles: Companies also generate inferred data – e.g. AI models may create an embedding or vector representation of your behavior or writing style. This technical data is used internally (for example, to predict your preferences or to detect policy violations in content). While not directly user-accessible, it’s part of what AI retains. User control: This is an area where external advocacy is pushing for transparency. Regulations like GDPR give users a right to access and delete personal data, which in theory includes these inferred profiles. In practice, companies do not yet offer a clear way to remove AI-learned embeddings related to you (especially if they are aggregated into a model). Advocacy is ongoing to establish whether model-trained data that came from personal info can be erased or not (the so-called “right to be forgotten” for AI).
User Control: Deletion, Restriction, and Transparency
Data Deletion Options: All major AI providers now offer at least basic tools for users to delete their data or account. For example, OpenAI has a privacy portal where users can delete their ChatGPT account and associated data (which triggers deletion within ~30 days) (How to delete your account | OpenAI Help Center). Google’s account settings let you delete Bard conversation history or set it to auto-purge after a period (How to Use Google Bard AI: A Step-by-Step Guide - GoZen.io), and similarly you can wipe Assistant voice logs. Meta allows deletion of your posts or your entire account (under GDPR you can also request data erasure). Amazon’s Alexa app allows deleting voice recordings one by one, by date range, or automatically going forward (Delete Alexa Voice Recordings Automatically - Amazon.com). However, deletion isn’t always comprehensive. As noted, Google warns that if a Bard conversation was sampled for review, it may be kept beyond deletion (though disassociated from your account) (Google's new Gemini-powered Bard raises privacy eyebrows). Amazon was found to retain text transcripts of audio after the audio was deleted (FTC Announces Enforcement Actions Against Amazon's Alexa and ...). Meta’s AI training on your data is hard to undo – if a photo you posted was used in model training, Meta doesn’t yet have a mechanism to selectively remove that influence. This has led to calls for “machine unlearning” techniques in the industry.

Opt-Out and Consent: Because outright deletion is tricky once data has been used, companies are starting to offer preventative controls – basically, the ability to opt out of your data being used for certain purposes in the first place. OpenAI’s ChatGPT history toggle is one such control (opting out of model training on your chats) (Stop OpenAI training its models on your chats by turning off history). OpenAI also provides a form for individuals or organizations to object to their data being included in training datasets, responding to GDPR’s right to object. Google’s Bard Activity off setting similarly opts you out of data retention beyond 72 hours (Teardown reveals Assistant with Bard is moving closer to Android ...). In late 2023, Meta added an opt-out form for EU users to exclude their social media content from Meta’s AI model training (Meta to Use Public User Data for AI Training, Allows EU Opt Out) (this was in response to European regulators, and is not available to most users outside the EU). These measures are a direct result of advocacy and legal pressure. Consent is another focus: AI firms are being pushed to obtain explicit consent before using sensitive personal data for training. For instance, using private communications or children’s data without consent has led to fines (as with Alexa and kids’ voices).

Transparency and User Awareness: Leading firms have published more transparency reports and privacy explanations for their AI. OpenAI now highlights how long they keep user prompts and for what (30 days for abuse monitoring) (Enterprise privacy at OpenAI). Google launched a “Privacy Hub” for Gemini AI (Gemini is their next-gen model) explaining data usage and that it doesn’t use conversations for ads (Google's new Gemini-powered Bard raises privacy eyebrows). Meta’s September 2023 “Privacy Matters” blog post for generative AI assured users that privacy was considered in their new AI features (though specifics were thin). Independent audits and reports have been crucial in this area. Academic and third-party audits have exposed discrepancies, such as the Alexa advertising study which revealed data uses not clearly told to users (Internet Privacy Laws Fall Short of Protecting People. A New ...). These findings pressure companies to be more upfront. In response, many now publish privacy whitepapers or responsible AI reports. Microsoft, for example, has detailed documents on how Copilot handles data and segregates it (Data, Privacy, and Security for Microsoft 365 Copilot).

Despite these steps, reliable user control is still evolving. It is generally effective for users to control surface-level records (deleting chat logs, voice clips, etc.), but controlling downstream uses of that data (like removing it from AI training) is much harder. Advocacy efforts by privacy groups and enforcement by regulators (FTC, European Data Protection authorities) are actively pushing companies to improve on this front. We see this in how Italy temporarily banned ChatGPT in 2023 until OpenAI added user controls (OpenAI gives in to Italy's data privacy demands, ending ChatGPT ban), and how EU pressure forced Meta to pause using EU data for AI until an opt-out was offered (Meta pauses AI development plans in Europe after privacy complaints).
Data Retention and Reuse Policies
Retention Periods: A key aspect of privacy is how long companies keep personal data. Recent practices show a trend toward limited retention for raw inputs but longer retention for derived data. OpenAI’s policy is to purge raw chat and API data after 30 days (unless you’re a paying enterprise who might get even stricter guarantees) (Enterprise privacy at OpenAI). Google’s default for Bard and other Web & App Activity is 18 months (How to Use Google Bard AI: A Step-by-Step Guide - GoZen.io), and Google even made 18 months the default auto-delete for new accounts. Google allows users to shorten this to 3 months or extend to 36 months (How to Use Google Bard AI: A Step-by-Step Guide - GoZen.io). Amazon did not have clear retention limits for Alexa data until pressured – by default it was “keep until deleted”. Now users can enforce a rolling deletion (e.g. keep only last 3 months of Alexa queries). Meta’s retention is typically “indefinite” for user profile data (they keep your info until you delete it, and even then some backup copies for a time). Meta hasn’t publicly committed to fixed deletion timelines for AI model training data – an area of concern since they could be retaining that indefinitely. Microsoft aligns more with enterprise needs: it will not retain customer prompts or content beyond service requirements. For example, if Bing chat logs are used to improve the service, Microsoft might anonymize and aggregate them and store those results, but personal identifiers are dropped after a time. Microsoft’s enterprise contracts often specify that data is deleted after a set period or at contract termination (Data processing addendum | OpenAI).

Reuse and Repurposing: Personal data collected by AI systems can be reused in various ways: model training, fine-tuning, feature development, or even sharing with third parties. Model Training: This has been a hot-button issue – OpenAI explicitly used ChatGPT user conversations to further train GPT-4 during its research preview, which raised flags. Now they allow opting out (Stop OpenAI training its models on your chats by turning off history). Google undoubtedly uses Bard interactions to refine its models (though details are scant, this is a reasonable assumption given their AI development pipeline). Meta clearly wants to reuse Facebook/Instagram content to develop foundation models (Meta to Use Public User Data for AI Training, Allows EU Opt Out). Secondary Uses: Amazon using Alexa data for ad targeting is an example of repurposing  (Internet Privacy Laws Fall Short of Protecting People. A New ...). Another example is voice data originally collected to help AI understand speech being later used to train a separate model (say, an accent detection model) – companies usually cover such reuse under broad privacy policy clauses. Policy Stance: Today, leading firms are careful to state that user data is used to “improve services” – this broad phrase encompasses AI training. Most have stopped short of selling personal data, but internal reuse is common. There is also a shift to de-identified data for AI training when possible. Google and Microsoft claim that any personal data used for improving AI is first stripped of identifiers and aggregated. Yet, privacy advocates note even de-identified data can be sensitive if it includes unique writing or voice patterns.

Regulatory Compliance: Regulations like GDPR (Europe) and CCPA (California) are forcing specific practices. Under GDPR’s data minimization and purpose limitation principles, companies must justify retaining data and using it for new purposes. This led OpenAI to implement shorter retention and the ability to object to training use in Europe. Meta’s planned reuse of data for AI without consent ran afoul of GDPR, leading to legal complaints (Meta faces call in EU not to use personal data for AI models | Reuters). As a result, Meta paused expanding some AI training efforts in Europe (Meta pauses AI models launch in Europe due to Irish request | Reuters). We also see companies publishing Data Protection Impact Assessments and allowing external audits. For instance, OpenAI said it welcomed audits after the Italy incident. Microsoft and Google undergo compliance audits for their cloud services, which include how they handle AI data. These trends suggest that industry standards are emerging around shorter data retention (measured in days or months, not years, for raw inputs) and giving users a say in whether their data can be reused for AI development.
Cloud-Based vs On-Device AI: A Shift for Privacy
One significant development influencing personal data handling is the movement from purely cloud-based AI to on-device AI. Traditionally, AI assistants and models ran in the cloud – meaning all user data (queries, voice, images) had to be sent to company servers for processing. This inherently exposed data to storage and potential misuse on those servers. Now, driven by both privacy concerns and technical advances, companies are moving some AI capabilities onto user devices:

On-Device Processing Benefits: When AI tasks run locally on your phone or computer, your raw personal data doesn’t need to leave the device. This gives users more control and privacy by design. Apple has been a leader here – for example, since iOS 15, Siri processes audio on-device by default for many requests, keeping your voice recordings off Apple’s servers. Apple also performs things like face recognition in Photos and keyboard autocomplete suggestions entirely on-device. While Apple isn’t explicitly in our list of AI companies, its influence has pushed others toward privacy-preserving AI.

Google’s Approach: Google has begun shifting parts of Google Assistant and other AI features to the device. The Pixel 7 and newer phones have on-device ML chips that handle speech recognition and understanding common commands without contacting Google’s servers (Google Assistant Uses On-Device AI for Pixel 7 and 7 ... - Voicebot.ai). Google’s Gboard keyboard uses federated learning – it learns from your typing habits on your device and only sends anonymized updates to improve the global model, rather than sending your entire typing history. At Google I/O 2023 and 2024, the company discussed efforts to compress large models (like the new Gemini AI) to run efficiently on devices, at least for specific tasks (Google I/O 2024: The Future of AI Assistants Unveiled). This Nano model trend aims to reduce dependency on constant cloud queries. Still, heavyweight tasks (like a full Bard conversation or complex image generation) remain cloud-based for now.

Meta’s Approach: Meta’s consumer apps largely run in the cloud, but Meta has also contributed to on-device AI in a different way: by open-sourcing models like Llama 2 that can be run locally. Enthusiasts have managed to run smaller versions of Llama on laptops and even smartphones, which means one could use a capable language model without sending data to Meta at all. Meta is also putting AI into devices like the Ray-Ban smart glasses (e.g., an AI assistant that can answer questions about what the glasses see). It’s not entirely clear how much of that processing is on-device versus cloud, but privacy considerations will likely push more on-device processing if possible (especially for camera data).

Amazon’s Approach: Amazon’s Alexa devices traditionally send voice audio to the cloud, but newer Echo models have tried to handle wake word detection and simple queries locally to speed up responses and limit data sent out. For instance, setting a timer or controlling a smart light might be processed on the Echo device itself. In 2023, Amazon announced an upgraded Alexa with a large language model; it’s expected to run primarily in the cloud due to its complexity. However, Amazon has expressed interest in Edge AI for IoT – meaning future Alexa interactions could be split, with your device doing as much as it can locally, only reaching the cloud for more complex tasks. This hybrid model could protect data by not sending every word you speak to Amazon’s servers (only the requests that truly need cloud intelligence).

Microsoft’s Approach: Microsoft leverages on-device AI in contexts like Windows and Office. Windows Hello (facial login) is on-device, and some AI features in Windows 11 (like background noise suppression in Teams calls) run locally. The new Windows Copilot, though, is essentially a cloud-powered assistant (Bing Chat) as of 2023, which means your prompts go to the cloud. In enterprise settings, Microsoft allows some AI processing within a customer’s private cloud or on-premises environment (e.g., Azure Cognitive Services can be deployed in a limited way on-prem). Moreover, Microsoft is working on optimizing models to run on specialized hardware (it has projects with Qualcomm to enable AI on Windows devices). So, while Microsoft’s consumer AI is still cloud-centric, they see on-device or on-customer-site AI as important for privacy and compliance (especially for business clients who don’t want data leaving their control).

Privacy Implications: The shift to on-device AI is a positive development for user privacy. If the model processes data locally, then data collection is minimized – the company might never see the raw input. This addresses many of the data retention and consent issues by preventing the data from being in the company’s possession in the first place. For example, if your phone can translate speech to text internally, you avoid sending audio to a cloud (where it could have been stored). On-device AI also reduces the risk of large-scale data breaches, since personal data isn’t all aggregated in a data center. Advocacy Impact: Privacy advocates have been encouraging this shift, arguing that not every AI task requires centralized data. Techniques like federated learning were in part driven by academic and public pressure to find privacy-friendly AI solutions. We’re seeing the fruits of that in products now. That said, on-device AI has limitations: devices have less computing power, so either models must be small or tasks simple. There’s a balance – for now, the most powerful AI models (ChatGPT, etc.) run in the cloud, so cloud privacy protections remain crucial. But moving ancillary or smaller tasks to the device is becoming industry standard when feasible.
Conclusion
Summary: Over the last two years, AI companies have made halting but notable progress in how they handle personal data. They’ve become more transparent about what they collect and how they use it, and introduced features to give users more control – from OpenAI letting users disable chat logging (Stop OpenAI training its models on your chats by turning off history), to Google adding an off-switch for Bard history (Teardown reveals Assistant with Bard is moving closer to Android ...), and Meta (under pressure) offering Europeans an opt-out for AI training (Meta to Use Public User Data for AI Training, Allows EU Opt Out). Data retention policies are slowly shrinking, with several firms now purging raw user data after a window (30 days, 18 months, etc.) (Enterprise privacy at OpenAI) (How to Use Google Bard AI: A Step-by-Step Guide - GoZen.io). However, challenges remain. Users still have to trust that companies actually delete or exclude their data as promised, and independent audits have sometimes found discrepancies (like Alexa voice data being repurposed for ads (Internet Privacy Laws Fall Short of Protecting People. A New ...)).

Advocacy and realistic user influence: The data types most critical for user control – conversation logs, voice recordings, and behavioral trackers – are exactly where advocacy has made a difference. Regulatory actions (e.g. Italy’s ban on ChatGPT, the FTC’s Alexa settlement) forced companies to implement opt-outs and shorter retention for chat and voice data. Privacy campaigners in the EU have leveraged GDPR to demand choices about training data, yielding the first opt-out mechanisms at Meta and others. Going forward, continued pressure is likely to expand these rights globally, so that users can truly choose whether their interactions feed AI models.

For now, users concerned about privacy should take advantage of the controls now available: turn off unnecessary data logging (such as Bard or ChatGPT history, or Alexa recording storage), periodically delete your AI histories, and use on-device modes when possible. The industry’s practices are trending toward greater respect for user data, but it’s the combination of user vigilance and strong advocacy that will ensure these companies follow through on privacy promises. Each of the leading AI firms has acknowledged that user trust is key – and handling personal data responsibly is central to maintaining that trust in the era of ubiquitous AI.

Sources:

OpenAI – data usage and retention: OpenAI help docs (data not used for training without opt-in; 30-day retention) (Enterprise privacy at OpenAI) (Does the openai API get access to the data I send it or store the data); OpenAI blog on user controls (Stop OpenAI training its models on your chats by turning off history).
Google – Bard and Assistant privacy: Google account help (Bard activity retention settings) (How to Use Google Bard AI: A Step-by-Step Guide - GoZen.io); News report on Bard privacy features (72-hour retention when history off) (Teardown reveals Assistant with Bard is moving closer to Android ...); Komando report on Bard data practices (Google's new Gemini-powered Bard raises privacy eyebrows).
Meta – use of personal data for AI: News of Meta privacy policy change (using user posts for AI training, EU opt-out only) (Meta to Use Public User Data for AI Training, Allows EU Opt Out); Reuters via Yahoo (NOYB complaints about lack of consent) (European Group Targets Meta's Plan to Use Personal Data for AI ...).
Amazon – Alexa data and user control: FTC complaint summary (retaining voice data despite deletion) (FTC Announces Enforcement Actions Against Amazon's Alexa and ...); PIRG/EFF on Alexa human reviewers and opt-out (Is Alexa always listening? How to protect your data from Amazon); UC Davis study on Alexa data for ads (Internet Privacy Laws Fall Short of Protecting People. A New ...).
Microsoft – enterprise AI data policy: Microsoft documentation (Copilot data not used to train foundation models) (Data, Privacy, and Security for Microsoft 365 Copilot); Microsoft community/press (promises of no use of customer content to train) (Copliot Privacy Concerns ++ - Microsoft Community).
On-device AI trend: Voicebot report (Google Assistant on-device on Pixel) (Google Assistant Uses On-Device AI for Pixel 7 and 7 ... - Voicebot.ai); ProfileTree tech explainer (Assistant requests processed locally for privacy) (What Is Google Assistant? + 7 Amazing Uses | ProfileTree); Aragon Research (Google I/O on on-device “Nano” models) (Google I/O 2024: The Future of AI Assistants Unveiled).


AI, Engagement & Addiction vs. AI for Good
In profit-driven digital platforms, artificial intelligence is often engineered to capture and hold our attention as long as possible. This “attention economy” design has sparked concerns about user engagement addiction – where algorithms manipulate our psychological triggers to maximize screen time. However, AI need not serve only commercial goals. It can be applied in education, civic life, and creative problem-solving in ways that prioritize user benefit over profit. Below, we explore how AI creates attention traps and their impacts, and contrast that with non-commercial AI applications, as well as efforts to promote more ethical, less addictive design.
AI-Driven Attention Traps
Profit-driven social media and video platforms use AI to maximize user retention and screen time. Advanced recommendation algorithms learn what keeps each user clicking, scrolling, or watching, and relentlessly serve up more of it. Key AI-driven features that act as “attention traps” include:

Algorithmic Feeds & Infinite Scroll: Instead of showing posts in chronological order, platforms like Facebook, Instagram, or TikTok use AI-curated feeds that never end. The feed continuously refreshes with personalized content tuned to the user’s interests and subconscious triggers. This design intentionally removes stopping cues – you can scroll indefinitely, always finding another engaging post. Studies have found that these algorithmic feeds markedly increase the time people spend on the platform. For example, an experiment during the 2020 U.S. election showed that when users were switched from the AI-curated feed to a simple chronological feed, they spent significantly less time on Facebook/Instagram (their total usage “substantially decreased”) (Social media and the 2020 election - Princeton University). In short, the endless, tailored feed is engineered to keep us hooked.

Autoplay & Continuous Recommendations: Video platforms like YouTube and Netflix leverage AI to automatically play the next video or episode, reducing the effort needed to keep watching. As one video finishes, the algorithm has already queued another likely to hold your attention. YouTube’s recommendation engine (the “Up Next” sidebar and autoplay) is famously effective at increasing watch time – by some accounts a large portion of viewing time on the site comes from these AI-driven suggestions. Autoplay exploits our tendency to consume passively; the path of least resistance is to keep watching, which the platform counts on to boost engagement metrics. Over time, the recommendation algorithms may even gravitate toward more sensational or emotionally charged content to prevent you from getting bored and leaving, according to tech insiders.

“Dopamine”-Triggering Design: Many of these features exploit our brain’s reward system to create habit loops. Tristan Harris (a former Google design ethicist) has likened the pull-to-refresh mechanism on apps (swiping down to refresh a feed) to pulling the lever of a slot machine – sometimes you get a rewarding new post, sometimes not, and that uncertainty fuels compulsive checking (Social media copies gambling methods 'to create psychological ...). Infinite scroll and algorithmic reward schedules function on the same principle of intermittent rewards, which are known to be highly addictive in psychology. As Harris put it, these apps are “unnervingly similar to a slot machine” in how they keep users hoping for the next payoff (Social media copies gambling methods 'to create psychological ...). In essence, AI systems curate content to spark mini dopamine releases in the brain (a funny meme here, an exciting video there) at just the right intervals to reinforce us to keep scrolling.

Collectively, these AI-driven design tricks turn apps into attention traps. The user’s own cognitive biases are leveraged against them: the feed always has something more, the video playlist never stops, and the unpredictable rewards make it hard to disengage. By maximizing time-on-site, platforms can serve more ads and harvest more data – directly tying these AI features to their profit model.
Psychological & Societal Effects
The rise of AI-optimized engagement has prompted extensive research into its psychological and societal impacts. Key findings from studies and experts include:

Diminished Attention Spans: There is mounting evidence that habitual use of these attention-grabbing platforms can shorten our ability to sustain attention. Cognitive researchers like Dr. Gloria Mark have tracked how digital media multitasking affects focus. In recent years, the average time a person spends focused on one screen task before switching is shockingly brief – on the order of 47 seconds on average (Our 47-Second Attention Span With Gloria Mark (S5: EP3) Transcript). This is a decline from a few years prior and reflects how constant notifications, feed updates, and quick-hit content have conditioned us to rapid shifts in attention. Social media’s stream of bite-sized posts and videos trains the brain to seek novelty continuously, potentially making it harder to concentrate on slower, more demanding tasks (like reading a long article or book). While multiple factors contribute to attention spans, researchers increasingly link excessive social media use with attention difficulties (Why our attention spans are shrinking, with Gloria Mark, PhD) (Wait, what? On Social Network Use and Attention). Essentially, an environment of endless micro-stimulation may be eroding our capacity for sustained focus.

Addiction & Mental Health Issues: Psychologists classify compulsive social platform use as a behavioral addiction for some users, driven by the same neural pathways as substance addictions. Brain imaging studies show that receiving social media rewards (such as “likes” on a post) activates the reward circuitry in the brain – notably the nucleus accumbens – in a manner similar to addictive substances like cocaine (Social media copies gambling methods 'to create psychological ...) (Social Media 'Likes' Impact Teens' Brains and Behavior). This constant activation of the brain’s pleasure centers can create dependency and craving: users feel a need to check apps repeatedly and experience anxiety when they can’t (sometimes called “digital withdrawal”). Numerous self-reports and clinical studies have noted symptoms of addiction – loss of control over usage, neglect of other activities, and distress when not using the platforms. Beyond addiction itself, there are correlated mental health impacts. Heavy social media use has been linked in longitudinal studies to higher rates of depression, anxiety, and loneliness, especially among young people. In an experimental study at the University of Pennsylvania, researchers found that when participants limited their social media use to ~30 minutes per day, they experienced significant reductions in loneliness and depression over just a few weeks compared to a control group (No More FOMO: Limiting Social Media Decreases Loneliness and ...). This suggests that the typical high usage levels (fueled by the engagement-driving AI algorithms) actively contribute to poorer well-being. Other studies have tied excessive social media consumption to issues like lower self-esteem (from constant social comparison), increased stress, and even “FoMO” (fear of missing out) anxiety. The cumulative mental health effect of AI-amplified social media has raised alarms in the public health community, prompting calls for viewing it as a potential health crisis for youth.

Social Fragmentation & Polarization: On a societal level, AI-driven content feeds can create echo chambers and polarization, contributing to social fragmentation. Because engagement algorithms tend to show users more of what they “like” and agree with, people may end up isolated in ideological or interest-based bubbles with little exposure to diverse viewpoints. Over time, this can reinforce partisan or extremist views, as the most engaging content is often that which provokes strong emotional reactions – outrage, fear, tribal loyalty. Researchers and policy analysts have pointed out that maximizing engagement often means amplifying divisive or extreme material (since mild or nuanced content doesn’t drive interaction as much). The Brookings Institute notes that “maximizing online engagement leads to increased polarization” and that platform algorithms are fundamentally designed in ways that amplify this effect (How tech platforms fuel U.S. political polarization and what ...). There is debate in academia about the extent of algorithms’ role in polarization – some 2023 studies (in Science) found that algorithmic feeds are not the sole cause of political polarization, indicating that human choices and network dynamics also play a big part (Social media algorithms are harmless, or are they? - AlgorithmWatch). Nonetheless, ample evidence exists of algorithmic radicalization pathways: for instance, YouTube’s recommendation engine in the past was observed pushing some users toward progressively more extreme content to keep them watching, and Facebook’s own research (revealed by whistleblowers) showed that its algorithms could inadvertently foster social division. Beyond politics, the constant fracturing of attention and curation of personalized realities can lead to a fragmented social fabric – a society where people no longer share a common information baseline. In summary, AI-tuned engagement systems, while successful at keeping us glued to our feeds, appear to have side effects of short-circuiting attention, contributing to addictive usage patterns, harming mental well-being, and splintering society along interest or belief lines.
Business Incentives & AI Manipulation
Why do companies design these hyper-engaging, potentially addictive systems? The answer lies in business incentives: most social media and consumer tech platforms rely on advertising revenue or data collection, which grow when users spend more time on the platform. Thus, they deliberately manipulate psychological triggers to maximize engagement. Some of the core techniques (rooted in behavioral psychology) include:

Intermittent Rewards (Variable Reinforcement): Platforms use intermittent reward schedules to keep users checking and rechecking. Not every post in your feed is great, and not every scroll yields something interesting – but once in a while, you hit upon a highly rewarding piece of content (a hilarious meme, a thrilling video, a juicy piece of news). This unpredictable reward pattern is the same principle that makes slot machines addictive. The user is conditioned to expect that “just one more refresh might bring something amazing,” which can lead to compulsive, repetitive behavior. Researchers have identified this intermittent reinforcement as a key factor in technology addiction, as it powerfully reinforces the habit loop (Why is Social Media Addictive? | Behavior & Research - Motley Rice) (Why You Can't Stop Checking Your Phone, According to Psychology). In practice, features like infinite scroll and algorithmic curation optimize the timing and frequency of these rewards. The user experience oscillates between moments of boredom and moments of delight, which is a recipe for forming habitual use. Tech insiders openly acknowledge borrowing from casino psychology: for example, the designer of the Facebook “Like” button has noted they considered how to make the feedback loop as engaging as possible, and Tristan Harris famously pointed out that the pull-down-to-refresh interaction was intentionally modeled on the randomness of a slot machine lever (Social media copies gambling methods 'to create psychological ...).

Social Validation Feedback: Humans are inherently social creatures who crave validation and fear rejection. Social media AI systems exploit this by creating feedback loops of social approval. The most obvious example is the “Like” (👍) or other reaction metrics on posts. Every time someone likes or comments on something you shared, the platform sends you a notification, pulling you back in with the promise of social reward. Psychologically, these likes trigger a release of dopamine (a pleasure chemical) as studies on teens’ brain activity have show (Social Media 'Likes' Impact Teens' Brains and Behavior)】. Getting positive engagement from peers lights up the brain’s reward centers, reinforcing the desire to post and interact more. Platforms knowingly gamify social interactions: users begin to chase likes, react to others for reciprocity, and engage in performative posting to gain validation. This can create an addictive feedback loop centered on social reward. The business incentive is clear: the more users engage with each other’s content (posting, liking, commenting), the more content there is to scroll and the more opportunities to show ads. Features like tagging, followers, and the display of view counts or “streaks” (see below) all serve to quantifiably encourage a user to seek status and validation on the platform. It’s a carefully engineered social feedback cycle: users get hooked on the mini high of peer approval that AI-mediated platforms deliver in doses.

Fear of Missing Out (FOMO): Another manipulation tactic is tapping into our fear of being left out or left behind. Platforms design their experiences to create a sense that if you don’t check in constantly, you might miss something important or fun. For instance, stories on Instagram or Snapchat are ephemeral (they disappear after 24 hours), which urges people to check frequently so they don’t miss friends’ updates. Similarly, push notifications are deployed strategically by AI – if you haven’t opened the app in a while, you might get a notification that “3 of your friends posted photos” or a vague teaser like “You have new notifications” to lure you back in. This leverages FOMO and curiosity. The recommendation algorithms also prioritize trending topics or breaking news that everyone is talking about, giving users a sense they need to stay logged on or risk being out of the loop. The “fear of missing out” drives many people to reflexively check their feeds whenever they have a free moment, and it’s strongly associated with *problematic social media use (Social Media and FOMO | Social Media Victims Law Center)】. One behavioral study noted that FOMO was a significant predictor of social media addiction, as users who feared missing events or updates engaged in more compulsive checkin (Social Media and FOMO | Social Media Victims Law Center)】. In short, companies know that if they can make their platform a must-check habit to avoid social exclusion, they will retain users. Features like read receipts (which tell when messages are seen) also pressure users to respond and stay continuously connected. By weaponizing our social anxieties and desire for inclusion, AI engagement models ensure we self-police our frequent usage.

Gamification & Streaks: Borrowing from game design, platforms introduce gamified elements that hook users via goals and rewards. A prime example is Snapchat’s “Snapstreak” feature – if two people send snaps to each other every day, a streak counter increases, accompanied by 🔥 emojis and even special trophies at milestones. This design leverages both reward seeking and loss aversion: users feel motivated to continue the streak to earn a higher count, and conversely fear the disappointment of breaking an ongoing streak. Some teens have reported feeling stress or obligation to maintain their streaks, sometimes giving friends access to their account when they can’t log in, just so the streak isn’t broken. This is a clear instance of a platform creating an extrinsic incentive (an arbitrary game reward) to boost daily active use. Research on streak psychology indicates that people will go out of their way to continue a streak once it’s started, which is “good for business” as one marketing study not (Duolingo, Inc.: Official Announcements - Stock Market News)9】. Beyond Snapchat, many apps use similar gamified metrics: Duolingo (a language learning app) shows your days-long learning streak, fitness apps count consecutive days of exercise, etc. While these can encourage positive habits, in a social media context they primarily encourage habitual platform return. The broader tactic is to turn user engagement into a sort of game – with scores, badges, or progress bars – thereby hooking into our competitive and goal-oriented instincts. When combined with AI-driven personalization, the platform can dynamically adjust challenges or reminders to keep each user highly engaged. All of this serves the business incentive of more frequent and longer sessions.

In summary, tech companies deliberately apply these psychological triggers in their AI-driven engagement models: by dangling intermittent rewards, providing social affirmation, instilling FOMO, and gamifying interaction, they manipulate user behavior in the service of keeping us online. These tactics are backed by extensive A/B testing and data – platforms constantly tweak their algorithms to find the precise reward timing or notification type that maximizes clicks and scrolls. The ethical line between persuasion and exploitation can be thin here; critics argue that many of these practices cross into manipulation of users’ vulnerable impulses (not unlike how a casino or slot machine operates). Yet as long as increased engagement equals increased profits, the incentive to use AI in these manipulative ways remains very strong in commercial platforms.
Constructive AI Beyond the Profit Motive
Not all applications of AI are geared toward trapping attention or boosting ad revenue. In contrast to the commercial engagement-maximization model, there are many constructive, community-oriented uses of AI that prioritize social benefit, education, or creativity over profit. These applications show how AI can function positively when designed with goals like learning, well-being, or civic value in mind, rather than monetization. Examples include:

AI in Education & Learning: A growing area is AI-powered personal tutors and learning assistants that adapt to students’ needs. These systems, often developed by nonprofits or research institutions, aim to improve learning outcomes instead of maximizing screen time. For instance, Khan Academy (a nonprofit education platform) has introduced an AI tutor named Khanmigo. Khanmigo uses a GPT-based AI to help students with problems and to assist teachers with lesson plans. Its design emphasis is on promoting deeper understanding and engagement with the material, not keeping the student on the site for arbitrary longer perio (AI tutor Khanmigo excels at asking questions, heralding possible ...)3】. Early reports note that the AI tutor asks guiding questions, gives hints, and encourages the learner – mimicking a good human tutor’s behavior. The focus is on mastery and curiosity rather than addictive usage. Similarly, universities and ed-tech researchers have built intelligent tutoring systems for subjects like math, science, and language, which personalize exercises and feedback. These AIs can identify where a student is struggling and provide targeted explanations or practice problems. Because many such projects are grant-funded or open-source, they are not driven by advertising and have little reason to deliberately prolong usage beyond what’s educationally beneficial. In fact, an effective tutor AI might help a student learn something faster, then sign off – a very different goal from the infinite engagement loop. Other educational AI applications include tools for special needs students (e.g. AI that converts speech to text for deaf students), automated grading assistants that free up teachers’ time, and tutoring chatbots that can answer questions 24/7. All of these illustrate AI’s potential as a positive, empowering tool when aligned with learning or personal growth goals rather than pure retention metrics.

Civic Engagement & Public Good: AI is also being harnessed to enhance civic life and community engagement in non-commercial contexts. One notable example is the use of AI to facilitate large-scale public discussions and consensus-building. In Taiwan, an online platform called vTaiwan has used an AI tool named Pol.is to help aggregate and analyze citizens’ opinions on policy issues. Pol.is uses machine learning to identify patterns in thousands of open-ended comments from citizens, clustering people who share similar views and highlighting points of consensus or division. This helps policymakers see where common ground lies in public opinion. The key is that Pol.is distills collective input without favoring the most emotionally charged or engagement-maximizing statements – in fact, it deliberately avoids highlighting the most contentious rhetoric, focusing instead on constructive feedback. This approach has been credited with improving dialogue on controversial issues by showing people areas of agreement. It’s an AI that depolarizes rather than amplifies conflict. The process invites citizens into an online space for debate, and the AI analysis helps ensure all voices are considered in aggregate, which politicians then use to inform decisio (vTaiwan - Crowdlaw for Congress)3】. This is quite the opposite of the engagement-driven social media dynamic – here AI is used to facilitate understanding and consensus rather than sensationalize. More broadly, AI is used in government and civic tech in ways that don’t center on profit: city administrations use AI to optimize public transport routes, allocate resources, or detect infrastructure issues; nonprofits use AI to analyze civic data (like mapping areas that need humanitarian aid); and citizen science projects employ AI to help volunteers sift through data (for example, identifying stars in telescope images or transcribing historical documents). These applications serve a social or public mission. Importantly, because they are not ad-funded, they don’t employ dark patterns to hook users – in fact, many civic apps would prefer to engage users just enough to get useful input or provide a service, and then let them go about their day. This shows AI can align with civic values like participation, transparency, and service.

Creative Problem-Solving & Research: Another arena for non-profit-motivated AI is in scientific research, creativity, and health. For example, DeepMind’s AlphaFold project (though developed by a private company, it was driven by a scientific grand challenge rather than user monetization) used AI to solve the 50-year-old problem of predicting protein structures. The resulting tool was donated to the scientific community, accelerating biomedical research worldwide. This kind of AI doesn’t even interact with consumers; it’s purely about crunching data to yield insights – a far cry from maximizing engagement. In healthcare, researchers have built AI systems to diagnose diseases from medical images or predict outbreaks by analyzing patterns, often with public health grants. These AIs are evaluated on accuracy and benefit, not on how long a doctor spends using the software. Similarly, AI is being used in climate science to model environmental changes or optimize renewable energy distribution. These projects, sometimes supported by government or philanthropy, highlight AI for social good. Even in the consumer realm, there are tools like mental health chatbots (some run by nonprofit foundations or research hospitals) that aim to provide counseling or support. Their success is measured by user well-being improvements, not minutes spent in the app. The Joseph Rowntree Foundation, for instance, emphasizes positive examples of AI such as in healthcare diagnostics, scientific research breakthroughs, and even AI-driven therapy or coaching, underscoring that AI can be harnessed to help people in tangible wa (Artificial intelligence for public good | Joseph Rowntree Foundation)9】. On the creative side, open-source AI projects allow artists, musicians, and writers to use AI to spark creativity – e.g. algorithms that generate music or assist in design – without any intention to lock in the user, but rather to hand them a novel tool. All these cases demonstrate a fundamental point: AI is a tool, and its impact depends on the goals we set for it. When the goal is profit via attention, we get addictive engagement loops; but when the goal is solving problems, educating, or enriching culture, we get AI that can augment human capabilities without exploiting our vulnerabilities.

In sum, there is a burgeoning ecosystem of “AI for good” – from education to civic tech to science – that operates outside the commercial attention economy. These initiatives typically prioritize user agency, privacy, and positive outcomes (learning, participation, discovery) over maximizing usage. They illustrate that AI’s tremendous power can be directed toward empowering users rather than manipulating them.
Regulation & Ethical AI Design
Given the concerns about AI-driven engagement addiction, there is increasing focus on regulating and redesigning these systems to be more ethical. Policymakers, researchers, and even technologists themselves are seeking ways to curb the manipulative aspects of engagement-centric AI. Several key approaches and initiatives include:

Government Regulation and Policy:  Authorities around the world are starting to address the harms of AI-boosted addictive design. In the European Union, the new Digital Services Act (DSA) directly tackles algorithmic recommendation systems. This law requires major platforms to be transparent about their algorithms and even mandates that users be given the option to use a feed not based on personalization (for example, a reverse-chronological timeline instead of a curated algorithmic fee (A guide to the Digital Services Act, the EU's new law to rein in Big Tech)4】. The idea is to grant users more control and to dilute the platforms’ ability to push whatever will drive engagement at all costs. The DSA also obliges platforms to assess and mitigate “systemic risks,” which include risks to mental health or civic discourse – effectively pressuring companies to consider and reduce addictive or polarizing algorithmic outcomes. In the United Kingdom, lawmakers have debated regulating “immersive and addictive technologies,” and the Age-Appropriate Design Code now in effect requires online services likely to be used by kids to consider the best interests of the child – this includes not nudging children to extend usage beyond what’s appropriate. In the United States, there have been legislative proposals and hearings on social media’s addictive features. While no federal law is in place yet specifically for algorithmic addiction, multiple states have taken legal action. Notably, in late 2023, 41 U.S. state attorneys general sued Meta (Facebook/Instagram’s parent company), alleging that the company knowingly designed its platforms to be addictive and harmful to teens’ mental heal (Meta sued by states claiming Instagram and Facebook cause harm ...)3】. The lawsuit claims that features such as infinite scroll, autoplay, and incessant notifications were engineered to hook youth, comparing the tactics to nicotine in tobacco products. This bipartisan legal action echoes growing public sentiment that Big Tech needs guardrails similar to other industries when it comes to protecting consumers (especially minors). We are seeing comparisons to historical regulation of addictive substances and activities, suggesting social media might require age limits, usage warnings, or design restrictions enforced by law. While it’s early, the trend is clear: regulators are no longer content with tech companies self-policing, and they are invoking consumer protection, privacy, and safety laws to push for healthier tech ecosystems.

Ethical Design Frameworks and Industry Initiatives: Alongside formal regulation, there is a movement within the tech industry and academia towards ethical AI design – creating guidelines and best practices to avoid exploitative patterns. Organizations like the Center for Humane Technology (founded by former tech insiders) advocate for “humane design” principles that put user well-being first. They provide resources and design guides for technologists to **minimize distractions and avoid triggering addiction (Resources | Humane by Design)1】. For example, a Humane Design Guide might suggest turning off endless scrolling by introducing natural stopping points, or switching the default from opt-out notifications to opt-in. The goal is to redesign interfaces so that they respect users’ time and attention rather than hijack it. There are also efforts in standards bodies: the IEEE has published guidelines on ethically aligned AI, and one of its recommended considerations is whether an AI system respects human autonomy and wellness. The ACM (Association for Computing Machinery) and other professional groups similarly call on developers to avoid creating algorithms that knowingly foster addiction or harm. Some tech companies have responded to public pressure with new features: for instance, apps and operating systems now often include digital well-being tools (like screen-time trackers, the ability to disable autoplay, “Take a break” reminders on YouTube, or grayscale phone display modes at night) to help users moderate their own behavior. Instagram and Facebook experimented with letting users hide like counts to reduce social comparison pressure. These are small steps, but they indicate an acknowledgment of the issue. On a larger scale, proposals exist for “algorithmic transparency” and “accountability” – requiring companies to allow audits of their AI systems or to implement changes if those systems are shown to cause harm (for example, if an algorithm is proven to significantly worsen teen mental health, regulators could mandate adjustments). Some scholars have argued that excessively “hyper-engaging” design could be treated as an unfair or deceptive practice under consumer protection l (Addictive Design as an Unfair Commercial Practice: The Case of ...)2】, which if adopted, would make manipulative interface features essentially illegal. While we are not there yet, the ethical technology community is actively researching and recommending such frameworks.

Public Awareness and Digital Literacy: Another important aspect of curbing AI-driven addiction is educating users. Media and nonprofit campaigns have helped raise awareness that if you’re using a free online service, “you are the product” – meaning your attention is being sold. Documentaries like The Social Dilemma (2020) popularized the idea that algorithms can manipulate us, which has empowered more people to take control by turning off recommendations, limiting notifications, or using apps more mindfully. Movements such as “Time Well Spent” (which later became the Center for Humane Technology) encourage users to reclaim their time and demand products that enhance well-being. In response, some platforms have begun to emphasize quality of engagement over quantity (at least in PR language) – for example, Facebook in 2018 tweaked its News Feed to show more posts from friends and fewer viral videos, claiming it wanted time on the platform to be “time well spent” rather than just maximized. Although skeptics note this may have been in anticipation of backlash, it shows the influence of public expectation on design. Governments and schools are also adding digital literacy programs to help people, especially youth, understand how algorithms work and how to recognize unhealthy usage patterns. Ultimately, an informed user base is harder to exploit; if users can identify that feeling of being manipulated by an app, they might disengage, which pressures companies to change tactics.

In conclusion, the issue of AI-driven engagement and addiction is now widely recognized, and a combination of regulatory pressure, ethical design advocacy, and user awareness is pushing the industry toward reform. Policies like the DSA in Europe and lawsuits in the U.S. represent a first attempt to rein in the most egregious engagement hacks, forcing platforms to be more transparent and giving users more choice. Ethical AI initiatives provide a blueprint for a different way of building tech – one that aligns with human values and long-term satisfaction rather than short-term clicks. The hope is that through these efforts, we can redirect AI development away from exploiting human weaknesses and toward augmenting human well-being. AI does not have to equal addiction – if guided by thoughtful design and oversight, it can be harnessed to enrich our lives without trapping us in endless engagement loops. The challenge and opportunity ahead is to create a digital environment where **technology works for us, instead of stealing our attention (787: Ethical Design of Humane Technology with Tristan Harris)1】, ensuring AI serves humanity in constructive ways.


Energy Consumption of Generative AI Models (Latest Data and Analysis)
Global Energy Footprint of Generative AI
Generative AI – including large language models (like ChatGPT, Bard) and image generators (like DALL-E, Midjourney) – runs on energy-hungry data center hardware. Quantifying AI’s exact energy use is challenging, but recent analyses give a sense of scale. In 2022, all data centers worldwide (including AI and crypto) consumed roughly 460 terawatt-hours (TWh) of electricity – about 2% of global electricity usage (What's the impact of artificial intelligence on energy demand?). Generative AI was only a portion of this total in 2022, but it expanded rapidly in 2023 with the explosion of AI adoption. One study estimated AI-specific workloads used ~5–9 TWh in 2023 (still a small fraction of all data center energy) (Generative AI: energy consumption soars - Polytechnique Insights), but this is rising quickly.

To put these figures in perspective: 460 TWh is roughly equivalent to the annual power consumption of a medium-sized country. If generative AI were a country, its 2023 electricity usage would likely be on the order of tens of TWh – comparable to nations like Ireland or Argentina (Generative AI: energy consumption soars - Polytechnique Insights) (Generative AI: energy consumption soars - Polytechnique Insights). For example, Google’s data centers used about 18.3 TWh in 2021, and AI-related computing was 10–15% of that (≈2.3 TWh) (Wild predictions of power demand from AI put industry on edge). By 2023, Google’s total data center load grew to ~24 TWh (Generative AI: energy consumption soars - Polytechnique Insights), reflecting the surge in AI activity. In short, generative AI already draws significant power, and its share of data center energy is climbing fast.
AI Power Use vs. Other Industries
Despite its growth, AI’s energy footprint remains small compared to major sectors:

Data Centers (incl. AI) – ~460 TWh in 2022 globally (≈2% of world electricity) (What's the impact of artificial intelligence on energy demand?). In terms of all energy (not just electricity), that’s under 0.5% of global final energy use.
Transportation – ~113 exajoules in 2021 (≈31,500 TWh, about 28% of global final energy) (Transport in Focus - REN21). This includes fuel for cars, trucks, planes, etc.
Industry/Manufacturing – ~30% of global energy use (Transport in Focus - REN21) (e.g. running factories, production plants).
Cloud Computing Giants – The biggest tech companies (Google, Amazon, Microsoft, Meta) together used ~72 TWh in 2021 (Global electricity consumption by AI could increase by 85-134 TWh ...). This exceeds the power usage of many countries (in 2023, Google and Microsoft each used more electricity than over 100 countries (Google and Microsoft's staggering electricity consumption was ...)). A growing portion of this is for AI services.

Today’s generative AI power usage is only a fraction of what sectors like transport or manufacturing consume, and even a subset of overall ICT (information and communications technology) energy use. For example, global ICT (data centers, networks, devices) is estimated around 2–4% of electricity use, whereas global transport energy is nearly an order of magnitude higher. So, AI is not (yet) one of the top energy-consuming industries (IEA: AI is 'not a primary driver' of global electricity demand). However, its relative share is rising quickly, and AI data centers are becoming a major new category of demand within the tech sector.

Another comparison: Traditional software vs. AI. Running a task on a generative AI model can use dozens of times more energy than using conventional software. One analysis found AI systems may use ~33× more energy to complete the same task compared to regular software (Ai's environmental impact: Energy consumption and water use). This highlights how computing with large neural models is far more power-intensive than past IT workloads.
Rapid Growth and Future Demand Projections
Energy demand from AI is growing at a breakneck pace, leading analysts to project a massive increase in coming years. Key forecasts include:

International Energy Agency (IEA) – Data centers (with AI) could double their electricity use by 2026. After consuming ~460 TWh in 2022, data centers’ annual consumption “could reach >1,000 TWh in 2026” (Executive summary – Electricity 2024 – Analysis - IEA). The IEA notes much of this growth is driven by AI and crypto. Even in a more conservative case, IEA sees data center demand surging significantly in just a few years.
S&P Global / Industry Analysts – By 2030, AI alone might account for 3–4% of global electricity. In macro terms, AI’s share of world power demand could rise to low-single-digits by 2030 (AI and ESG – A twisted tale or made for each other - Times of India). That would be a multi-fold increase from today (well under 1%).
United States DOE Report (2024) – U.S. data centers’ energy use is climbing steeply. Data centers went from using ~1.9% of U.S. electricity in 2018 to 4.4% in 2023 (Data center energy demand could triple by 2028 — DOE - E&E News) (about 176 TWh). With AI adoption, US data center use could reach 6.7–12% of U.S. electricity by 2028 (US data-center power use could nearly triple by 2028, DOE-backed ...) – meaning it may double or triple again in five years. This DOE-backed study signals an inflection point, largely due to AI workloads driving new demand.
IDC Industry Forecast – AI-specific data center energy is growing ~45% annually. At this rate, AI data centers would draw ~146 TWh by 2027 (IDC Report Reveals AI-Driven Growth in Datacenter Energy ...) (globally). That’s up from only a few tens of TWh in the early 2020s – a very sharp trajectory.
Individual Companies – Tech firms are dramatically scaling infrastructure for AI. For example, Google’s AI computing load grew ~3× from 2019 to 2021 (Artificial Intelligence's Impact on Power Demand Gets Closer Look), and Microsoft reports similar trends. NVIDIA’s sales of AI chips (GPUs) are booming, which implies many new AI servers coming online – each server drawing kilowatts of power. Estimates based on chip shipments suggest global AI electricity use could jump by 85–134 TWh/yr by 2027 vs. current levels (Global electricity consumption by AI could increase by 85-134 TWh ...).

In summary, all projections show AI’s energy appetite rising orders of magnitude. By the end of this decade, running AI models could be consuming hundreds of TWh per year globally, moving AI from a niche consumer into a major slice of power demand. The exact numbers vary by scenario – but the consensus is that if AI adoption keeps accelerating, energy use will follow suit. This has prompted urgent discussions about how to power the coming “AI boom” sustainably.
Energy Sources for AI Infrastructure and Environmental Impact
What powers all this AI? Mostly electricity from the grid – which today still comes largely from fossil fuels globally. As of the early 2020s, roughly 60% of world electricity is generated by coal, gas, and oil (with the remainder from renewables and nuclear) (Executive summary – Electricity 2024 – Analysis - IEA). This means that AI’s electricity consumption has a significant carbon footprint unless it’s directly supplied by low-carbon power. For example, if an AI data center draws grid power at the global average mix (~0.5 kg CO₂ per kWh), a 100 TWh/yr AI load would emit on the order of 50 million metric tons of CO₂ annually. (For scale, that’s like adding the emissions of a mid-size country.)

However, the picture is nuanced: many leading AI/cloud companies are actively greening their energy supply. Major firms like Google, Microsoft, Amazon, and Meta have committed to 100% renewable electricity for their data centers, at least on an annual matching basis. Google, for instance, has matched 100% of its global power use with renewable energy purchases every year since 2017 (24/7 Clean Energy – Data Centers - Google). In 2023, Google achieved this for the 7th year running. Microsoft and Amazon also buy large amounts of wind and solar to offset their data center usage.

Despite these efforts, rapid growth in AI demand is straining sustainability goals. It’s one thing to offset 10 TWh of usage with renewables, but if a company’s AI needs jump to 30+ TWh, securing additional carbon-free energy at that scale is challenging. There are concerns that new AI data centers may outpace the build-out of clean energy. For instance, local governments (like in Ireland and the Netherlands) have at times restricted new data centers because their huge electricity draw could conflict with climate targets (AI processing could consume 'as much electricity as Ireland'). A recent study warned that Google’s AI expansion alone could push its power use toward the scale of Ireland’s entire grid consumption (~30 TWh/yr) (Generative AI: energy consumption soars - Polytechnique Insights) (Generative AI: energy consumption soars - Polytechnique Insights), underscoring the pressure on power infrastructure.

From an environmental cost perspective beyond carbon, AI data centers also consume water and resources. Big server farms require cooling – often using evaporative cooling or chillers – which can use millions of gallons of water per year for a large facility (As Use of A.I. Soars, So Does the Energy and Water It Requires). So a spike in AI computation can mean more water drawn from local supplies (though companies are exploring liquid cooling and other methods to reduce this). The manufacturing of AI hardware (GPUs, chips) is energy-intensive and has its own upstream emissions too.

On the positive side, AI can help optimize energy use and reduce emissions in other domains. For example, AI-driven data center cooling has shown impressive gains – Google’s DeepMind AI controls cut data center cooling electricity by 30% using intelligent adjustments (As Use of A.I. Soars, So Does the Energy and Water It Requires). More broadly, AI is being used to model building energy use, optimize industrial processes, and manage electric grids more efficiently (AI and energy: Will AI reduce emissions or increase demand?). These applications can save energy and lower emissions, potentially offsetting some of AI’s direct power draw. One estimate suggests “AI-run smart homes could reduce household CO₂ emissions by up to 40%” through intelligent climate control and efficiency measures (As Use of A.I. Soars, So Does the Energy and Water It Requires). In short, AI is both an energy consumer and a tool for energy savings – its net impact will depend on how it’s deployed and powered.
Future Trajectory: Toward Nuclear-Scale Power (or a Dyson Sphere?)
If generative AI growth continues unabated, will we need new power sources on the scale of nuclear plants – or even a sci-fi Dyson Sphere – to keep it running? It’s a provocative question that underscores just how energy-hungry advanced AI could become. While a Dyson Sphere (a megastructure capturing an entire star’s energy output) remains purely hypothetical, some experts have openly mused that unbounded AI expansion would eventually hit fundamental energy limits. In practical terms, leaders in AI are already looking to nuclear energy as a key part of the solution.

Notably, OpenAI’s CEO Sam Altman has emphasized that major breakthroughs in energy are needed for the future of AI. At a 2024 forum he said “an energy breakthrough is necessary for future AI, which will consume vastly more power than the current generation” (OpenAI CEO Altman says at Davos future AI depends on ... - Reuters). Altman has put his money behind this sentiment – he chairs a nuclear startup (Oklo) and personally invested in fusion energy (Helion) (OpenAI CEO Altman says at Davos future AI depends on ... - Reuters). In fact, OpenAI (and its partner Microsoft) are reportedly planning massive new data centers on the order of 5 gigawatts of power capacity by the end of the decade (How to Build the Future of AI in the United States | IFP). Five GW is equivalent to five large nuclear reactors or about 5 million homes’ worth of demand – a truly “nuclear-scale” ambition for AI infrastructure. Microsoft has even signed a deal to buy 50 MW of fusion power by 2028 (from Helion Energy) to directly supply its data centers (Microsoft signs 50MW fusion power PPA with Helion for 2028 - DCD), indicating how seriously the industry is pursuing advanced energy sources.

So, in the foreseeable future, scaling up AI may go hand-in-hand with scaling up nuclear and renewable power. We likely won’t need a literal Dyson Sphere (capturing the Sun’s full output) anytime soon – AI would have to grow billions-fold for that – but the comparison highlights a real challenge: unlimited computational growth meets finite planetary resources. If AI demand ever approached even a few percent of global energy, it would force tough choices or radical innovations in energy supply.

Fortunately, there is substantial room to maneuver before we hit science-fiction levels. Efficiency improvements in AI are expected to continue: better chips (ASICs, photonic processors), optimized algorithms, and techniques like model quantization can deliver more AI output per watt of power. Researchers have demonstrated methods to cut AI training energy by 80% with smarter algorithms and hardware co-design (New tools are available to help reduce the energy that AI models ...). Such gains will be crucial to prevent energy use from skyrocketing exponentially. Additionally, the shift toward sustainable energy means that even if AI uses a lot of power, it can be low-carbon power if trends in solar, wind, and nuclear deployment continue.
Key Insights and Broader Impacts
Generative AI currently uses a noticeable but not yet enormous share of global energy. All AI/data centers combined are on the order of 1–2% of world electricity use (What's the impact of artificial intelligence on energy demand?), far less than sectors like transport (~28% of energy) or industry (~30%) (Transport in Focus - REN21). But AI’s slice is growing fast and could reach a few percent of global electricity within a decade (AI and ESG – A twisted tale or made for each other - Times of India).

The energy needed for AI is growing exponentially. Short-term projections (to 2025–2030) show AI data center demand possibly doubling every 3–4 years (Executive summary – Electricity 2024 – Analysis - IEA). Without intervention, this trend would mean AI workloads consume an increasingly significant chunk of grid capacity, necessitating major investments in power generation and grid infrastructure.

Powering AI is becoming an energy industry issue, not just an IT issue. The scale of data centers being planned (gigawatt-scale campuses (How to Build the Future of AI in the United States | IFP)) and deals for dedicated power (e.g. fusion in 2028 (Microsoft signs 50MW fusion power PPA with Helion for 2028 - DCD)) signal that electricity providers and policymakers must account for AI’s rise. Some regions are already wary of the grid stress and emissions impact from new AI superclusters (AI processing could consume 'as much electricity as Ireland').

The source of electricity matters hugely for AI’s environmental impact. If AI is powered by coal-heavy grids, its carbon footprint will balloon; if powered by renewables or nuclear, the same electrical growth can be achieved with minimal emissions. The tech sector’s push toward renewables is encouraging – e.g. matching usage with wind/solar purchases – but additionality and 24/7 clean power will be key. Ensuring AI’s new loads are met with new clean energy (not just credits) is a challenge ahead.

Efficiency and innovation can mitigate the energy surge. On the supply side, deploying more carbon-free generation (solar farms, wind parks, SMR nuclear reactors) will support AI sustainably. On the demand side, AI engineers are pursuing “Green AI” strategies: model compression, more efficient architectures, and better cooling and power management in data centers. The compute-per-energy of AI has improved in recent years (for instance, training the same model now uses much less energy than a few years ago due to such advances (New tools are available to help reduce the energy that AI models ...)), helping to blunt the impact of larger models.

AI’s broader impact on sustainability cuts both ways. While AI uses energy, it can also be applied to save energy elsewhere. For example, AI algorithms optimize smart grids, improve industrial energy efficiency, and enable smarter transportation logistics – all reducing waste (AI and energy: Will AI reduce emissions or increase demand?) (The role of AI in energy consumption - Ericsson). There is a real opportunity for AI to be a net positive for the climate if used to accelerate energy efficiencies and renewable integration in other sectors. The key will be balancing AI’s direct footprint with these indirect benefits.

In conclusion, the power consumption of generative AI is no longer an academic footnote – it’s a growing factor in global energy planning. Today it’s on the order of a few tenths of a percent of world energy, but its trajectory is steep. Ensuring that this transformative technology can scale without unsustainable energy burdens will require concerted effort: from deploying clean energy at scale, to innovating in compute efficiency, to perhaps rethinking algorithmic approaches. The coming decade will reveal whether AI becomes a driver of clean-energy innovation or a new strain on the electric grid – and proactive steps taken now will heavily influence that outcome (OpenAI CEO Altman says at Davos future AI depends on ... - Reuters) (AI and energy: Will AI reduce emissions or increase demand?).

Sources:

International Energy Agency (IEA) data on data center energy use (Executive summary – Electricity 2024 – Analysis - IEA) (What's the impact of artificial intelligence on energy demand?)
U.S. Department of Energy & Lawrence Berkeley National Lab report on data center power (2024) (Data center energy demand could triple by 2028 — DOE - E&E News) (US data-center power use could nearly triple by 2028, DOE-backed ...)
S&P Global and industry analysis on AI’s share of future power demand (AI and ESG – A twisted tale or made for each other - Times of India) (Wild predictions of power demand from AI put industry on edge)
Academic and technical studies (e.g. Alex de Vries in Joule, IDC report) estimating AI energy growth (Global electricity consumption by AI could increase by 85-134 TWh ...) (IDC Report Reveals AI-Driven Growth in Datacenter Energy ...)
Tech company disclosures (Google energy reports, etc.) and case studies on AI efficiency and renewable use (Wild predictions of power demand from AI put industry on edge) (24/7 Clean Energy – Data Centers - Google)
World Energy Council / REN21 statistics for sectoral energy comparisons (Transport in Focus - REN21)
Reporting from Reuters, Fortune, etc., on AI data center projects and energy deals (OpenAI, Microsoft) (OpenAI CEO Altman says at Davos future AI depends on ... - Reuters) (Microsoft signs 50MW fusion power PPA with Helion for 2028 - DCD).


Partial "Open Source" AI and Global Access to Innovation
The Reality of "Open Source" AI
Many AI models are branded as "open source" while withholding crucial components like full model weights, training data, or broad usage rights. In true open-source software, all components are freely available under a permissive license, but some AI releases fall short of this standard (Meta's LLaMa 2 license is not Open Source). For example, Meta’s LLaMA 2 model was touted as open, yet its license restricts certain uses – including a clause forbidding some commercial applications and even using LLaMA outputs to train other models (Llama FAQs). According to open-source advocates, such restrictions mean these models are not truly open in the way the term is traditionally defined (Meta's LLaMa 2 license is not Open Source).

This partial openness has real impacts on who can benefit. If model weights or training datasets aren’t fully released, it becomes difficult for researchers or developers outside big institutions to adapt the AI to local needs. Red Hat’s perspective is that “open source AI” should at minimum include open-licensed model weights along with the code (Open source AI: Red Hat's point-of-view). Others argue that openness must extend to training data as well: releasing only code or a model without the data it learned from leaves significant blind spots, making it hard to understand or correct the model’s behavior (Open source AI and open data – Blog Fiasco). When crucial pieces are withheld, communities around the world cannot fully fine-tune or modify AI models to address local languages or niche applications. In effect, “open” AI that isn’t completely open can limit participation to those with privileged access, reinforcing existing inequalities in who can innovate with AI.
Power Dynamics in AI Development
Today’s AI development is highly centralized, dominated by a handful of tech giants and wealthy states. Companies like OpenAI (with Microsoft), Google (Alphabet), Meta (Facebook), and a few others command enormous computing resources and talent pools, as do major Chinese firms like Baidu, Tencent, and Alibaba. This concentration raises concerns that the market for foundation models could tilt toward a monopoly or oligopoly structure (Competition, Innovation, and Research). In fact, U.S. analysts have warned that AI foundation model markets are already exhibiting tendencies toward concentration in the hands of a few players (Competition, Innovation, and Research). The result is a power imbalance: a small club of organizations (mostly in the US and China) are setting the agenda for AI research and deploying the most advanced models, while others are largely consumers of these technologies.

Such centralization has far-reaching implications for governance and access. If only a few entities control the most powerful AI models, they also wield outsized influence over what norms and safety standards are implemented – and whose values are reflected. These dominant players can shape global AI governance to suit their interests, potentially sidelining the priorities of smaller nations or independent researchers. Experts caution that without a more inclusive approach, we risk widening the global AI divide, leaving emerging economies and less powerful communities behind (China's AI Governance: Engaging the Global South). Indeed, the economic and social benefits of AI are currently concentrated in the Global North, while many countries in the Global South may end up accessing AI “in a secondary manner” – using systems developed elsewhere rather than leading development (AI Innovation Concentration and the Governance Challenge - Issuu) ([PDF] responsible artificial intelligence efforts in the global south). This dynamic can entrench dependency: countries outside the AI superpowers might have little choice but to adopt tools from the major providers, on those providers’ terms.

In summary, the concentration of AI development among a few big players creates an uneven power dynamic. Those few entities (and the countries backing them) gain disproportionate control over the trajectory of AI innovation, the direction of research, and the terms of access for everyone else. This raises questions about how to ensure future AI governance includes broader representation and how other nations can maintain some autonomy and influence in the AI space.
Barriers for Local AI Adaptation
Many communities – especially in the Global South – have unique needs that generic, globally-trained AI models do not address well. Languages, dialects, and local context vary widely, but current large language models are heavily skewed toward data in English and a few other major languages. In fact, an estimated 88% of the world’s languages are insufficiently represented in the data on the internet, which means most languages barely figure into AI training sets (Contextualizing Large Language Models in Southeast Asia). This leaves huge populations (potentially 20% or more of the world) at risk of being overlooked by AI systems that don’t “speak” or understand their language and culture. For example, farmers or doctors in sub-Saharan Africa or rural South Asia might benefit from AI models tailored to local crops or diseases and conversing in local languages – but such models require adaptation that current mainstream AI is not providing.

Adapting AI for local contexts often means fine-tuning a model on additional data or training it to handle a new language or domain. Here is where licensing and access barriers become problematic for less-resourced communities:

Restricted Model Access: If a state-of-the-art model’s weights are not openly released, local developers simply cannot build on it. They might be limited to using an API (if available), which doesn’t allow them to retrain or modify the model for things like regional dialects or domain-specific knowledge. Training a new model from scratch is usually beyond the reach of universities or startups in developing countries due to the enormous compute and data required.

License Constraints: Even when model weights are released, licenses can bar certain uses. For instance, Meta’s LLaMA was released for research but prohibited commercial use by some users (Meta's LLaMa 2 license is not Open Source). A startup or government agency in the Global South looking to deploy an AI service (say, a local-language educational tutor or an agricultural assistant) might find that they legally can’t use a so-called “open” model for this purpose without negotiating rights. Similarly, clauses like LLaMA 2’s rule against using its outputs to train other models (Llama FAQs) hinder iterative improvement – a local team can’t even leverage LLaMA’s answers to help train a more culturally-specific model. These limitations can freeze out local innovation, or force communities to rely on whatever the original model provides (even if it’s not a great fit locally).

Computing and Data Resources: Beyond licenses, many Global South communities face practical barriers such as limited computing infrastructure and scarce training data in local languages (How to Make AI Equitable in the Global South). Major AI firms can scrape and use vast text datasets from the internet, but content about and by smaller communities may be minimal online. As a result, foundation models often lack knowledge of local customs or vocabularies (How to Make AI Equitable in the Global South). When those models are closed, outsiders can’t even examine the training data to see what’s missing. Open data initiatives and multilingual projects (like the Bloom LLM which covers 46 languages) are trying to fill this gap, but they are the exception rather than the norm.

In effect, current licensing models and restricted releases act as gatekeepers. Communities in Africa, Latin America, Southeast Asia and elsewhere have the talent and the use cases for AI, but they may not have legal access to the best models to adapt for public services, education, or local business. This is why there is a push for truly open-source AI solutions – so that a teacher in Kenya or an agriculture extension worker in India can take a base model and fine-tune it to directly address their community’s needs without undue hurdles (Global South's role in AI governance explored at IGF 2024). Some recent partnerships illustrate both the need and the challenge: for example, a French telecom company (Orange) has partnered with OpenAI and Meta to fine-tune language models for African languages (Orange Partners With OpenAI To Make LLMs More Inclusive In Africa), an initiative aimed at digital inclusion. The fact that such a partnership (involving corporate permission and support) is needed underscores how difficult it currently is for local actors to do this on their own.
Verification and Auditing Challenges
When AI models are not fully open, it’s not just development that suffers – oversight does too. Independent researchers, ethicists, and community leaders need transparency to verify how an AI model works, check for biases, assess safety, and ensure it aligns with societal values. If the model’s inner workings (its architecture, weights, and training data) are kept secret, auditors are essentially working with a black box. They can observe inputs and outputs, but cannot inspect the mechanism generating those outputs.

This opacity makes it hard to identify problematic behaviors or biases systematically. For instance, detecting bias in how a model treats different languages or demographic groups might require analyzing its training data or weight distributions – impossible without access. One policy analysis notes that releasing model weights can actually improve safety by empowering independent researchers to uncover issues that the model creators might have missed (Beyond Open vs. Closed: Emerging Consensus and Key Questions ...). In other words, openness enables external audits: academics and activists can stress-test the model, find biases or vulnerabilities, and even suggest fixes. Conversely, with a closed model, we are often forced to take the company’s word on its safety or rely on very limited probing from the outside.

There have been real-world examples where lack of transparency led to harm. A often-cited case is the COMPAS algorithm used in the U.S. justice system to predict recidivism (likelihood of re-offending). COMPAS was proprietary, its model and data closed off from public scrutiny. It wasn’t until investigative journalists obtained output data and analyzed it that a major bias was discovered: “blacks are almost twice as likely as whites to be labeled a higher risk but not actually re-offend,” the ProPublica team found (Racist Technology in Action: Predicting future criminals with a bias ...). This kind of racial bias in a decision-making algorithm had serious consequences (affecting sentencing and bail decisions), and it went unchecked for years partly because independent experts couldn’t audit the system easily. Only after exposure did the issue come to light, sparking debates on fairness. The COMPAS case underlines the danger of “trust us, it’s fine” approaches – without transparency, flawed or biased AI can proliferate and cause real harm before anyone is able to verify and correct it.

Even with less high-stakes applications, closed models pose challenges. If a community suspects that a language model consistently gives prejudiced or unsafe outputs about them, how can they prove it or push for change if everything about the model is secret? Transparency is crucial for accountability. As one research summary puts it, without openness it becomes difficult to audit AI systems, hold developers accountable, or empower users to understand how decisions are made (Ethical Implications of AI: Bias, Fairness, and Transparency). This is why open-source AI advocates stress not just open code, but open weights and training datasets – so that claims of bias, safety, and alignment can be independently verified. When models are partially closed, society has to rely on the goodwill and thoroughness of the small group who built the model, which is not a robust governance strategy. Documented incidents like COMPAS have prompted calls for greater AI transparency to prevent “black box” algorithms from harming communities.
The Rise of Open Source AI Alternatives
In response to the dominance of proprietary AI, a growing movement is pushing for open-source or openly available AI models. Several high-profile models and projects have emerged as alternatives to the closed systems from the tech giants. These open or semi-open models aim to democratize access to advanced AI capabilities. Notable examples include:

Meta’s LLaMA Family: Meta (Facebook) made waves by releasing the LLaMA series of large language models to researchers. The LLaMA 2 model, with up to 70B parameters, was made available for free use with certain conditions. While its license is not fully open-source by strict definition (due to usage restrictions) (Meta's LLaMa 2 license is not Open Source), it still allowed a broad community of academics and developers to download the model weights and fine-tune them. This led to a proliferation of community-built chatbots and domain-specific LLMs based on LLaMA 2. Meta has continued this strategy; for instance, it announced Llama 3.1 (a 405B-parameter model) as “the world’s largest and most capable openly available” model (Introducing Llama 3.1: Our most capable models to date - AI at Meta). By releasing such models, Meta is challenging the idea that only closed models can be state-of-the-art – and is leveraging the open community to improve and adapt its AI. However, it’s worth noting Meta’s license still forbids certain uses (like using LLaMA outputs to train competing models) (Llama FAQs), indicating a balance between openness and corporate interest.

Stability AI’s Models (StableLM & Stable Diffusion): Stability AI is a startup that has championed fully open AI releases. They funded and released Stable Diffusion in 2022, an image generation model that quickly became the backbone for countless creative tools worldwide, precisely because anyone could use or modify it. Building on that ethos, Stability AI also launched StableLM, a series of open-source language models. The initial versions (3B and 7B parameters) were released under a permissive Creative Commons license (CC BY-SA 4.0), explicitly as an “open source ChatGPT alternative” (Stability AI launches StableLM, an open source ChatGPT alternative). While StableLM’s smaller models cannot match the performance of giant models like GPT-4, the key is that they are transparent and adaptable. Developers anywhere can fine-tune StableLM for their needs, or even inspect how it was trained. Stability AI’s CEO has argued that open models and datasets will improve safety and foster competition, benefiting the AI ecosystem as a whole (Stability AI CEO advocates for open models in AI oversight - AiHunt).

Mistral AI’s Models: Mistral AI, a newer Europe-based startup, has released models such as Mistral 7B, focusing on making high-quality open-weight models available. “Open-weight” means the trained weight files are downloadable by anyone, enabling independent deployment. Such open-weight models are “indispensable tools for open innovation, allowing anyone to download and deploy cutting-edge AI models independently,” as one report noted (Open Weight Definition (OWD): Delivering Clarity while Protecting ...). By open-sourcing their model weights (often under Apache or other permissive licenses), Mistral and similar companies are providing genuine alternatives to corporate AI APIs. Their 7B model reportedly matches or exceeds the capability of much larger models in certain tasks, demonstrating the potential of community-driven optimization.

BigScience BLOOM: BLOOM is a landmark example of an open-science collaboration producing an open AI model. Organized by a global consortium of researchers (with backing from organizations like Hugging Face), BigScience released BLOOM, a 176-billion-parameter multilingual language model open to all (A 176B-Parameter Open-Access Multilingual Language Model - arXiv). BLOOM was trained on 46 languages and 13 programming languages, explicitly including many underrepresented languages. It was a proof-of-concept that a diverse team outside the corporate world could create a top-tier model and release it openly. BLOOM’s open-access model allows any researcher or developer to use and study it, which particularly helps communities whose languages were included (e.g. African and South Asian languages often missing from other LLMs).

These efforts collectively show that open alternatives are rising. They challenge proprietary models by offering similar capabilities with fewer barriers. In some areas (like image generation with Stable Diffusion), open models have arguably overtaken closed ones in adoption and innovation speed. In other areas, open models are rapidly closing the performance gap – for instance, recent open-source LLMs have approached the quality of leading proprietary systems (Venture Bytes #110: Open Source LLMs Leveling the AI Playing Field). An analysis in late 2024 noted that open-source LLMs were “rapidly closing the performance gap with closed models and even getting better in some cases,” pointing to Meta’s latest Llama as evidence (Venture Bytes #110: Open Source LLMs Leveling the AI Playing Field).

However, it’s also clear that many of these “open” models still rely on corporate or institutional support. True community-led models (without big funding) remain relatively small in scale. Open models often leverage resources that only large players can afford (Meta’s compute, Stability’s funding, etc.), which means the landscape still favours those with power. Additionally, not all “open” releases are equal – some, like LLaMA, come with strings attached, and even fully open models may not include the training dataset for copyright or privacy reasons. So while models like LLaMA, StableLM, BLOOM, and Mistral’s offerings do provide genuine alternatives that chip away at corporate dominance, there are still limitations. In some cases, the highest performing AI (e.g. GPT-4) is still proprietary and inaccessible, which means open models have more work to do to completely level the playing field. Nonetheless, the rise of open-source AI has begun to democratize development: it’s now possible for a startup in Indonesia or a research lab in Kenya to take an open model and build something locally useful, which was far harder when only closed models existed.
Future AI Governance and Control
Looking ahead, if the status quo does not change and only a few companies or countries retain control over the most advanced AI models, there could be significant consequences for global innovation, economic balance, and ethical norms. Many experts speak of an emerging AI divide analogous to the digital divide, where the benefits of AI are unevenly distributed. Currently, leading AI nations like the US and China are racing ahead, and “most countries will access AI in a secondary manner” if they cannot develop their own foundation models (AI Innovation Concentration and the Governance Challenge - Issuu). Here are some key implications of a future where AI control remains highly concentrated:

Global Innovation Gaps: Innovation would cluster around the AI superpowers. Nations without access to core AI technology might become perpetually dependent on imported AI solutions. This could stifle local tech sectors. For example, if cutting-edge AI models are only developed in Silicon Valley or Beijing, researchers in other regions may always be a step behind, only applying what’s handed to them. The creativity and problem-solving that come from adapting technology to local contexts might suffer. A policy paper from the World Economic Forum noted that the “AI divide” risks leaving the Global South behind as the Global North reaps most of AI's rewards (The 'AI divide' between the Global North and Global South). Without deliberate efforts to share AI knowledge and tools, global innovation will be narrower, driven by the perspectives and interests of a few power centers.

Economic Inequality and Competition: AI is poised to be a major driver of economic growth, productivity, and new industries. If only a handful of companies/countries own the top AI models, they are likely to capture outsized economic gains. This could widen global inequality – the wealth generated by AI could accrue mostly to those AI hubs. Other countries might see slower growth or have to pay hefty fees to use AI developed abroad, acting as consumers rather than producers of the next tech revolution ([PDF] responsible artificial intelligence efforts in the global south). Moreover, countries may feel a loss of competitive sovereignty. In the way that oil-rich countries had leverage in the 20th century, AI-rich entities might dominate the 21st. Some have drawn parallels to a new kind of dependency, even using terms like “AI colonialism” to describe how data and AI capabilities are extracted from the Global South but controlled by the Global North (The Movement to Decolonize AI: Centering Dignity Over Dependency). If only the US and China (and a few Big Tech firms) have the strongest AI, other nations could effectively become locked out of competition in key industries (from advanced manufacturing to pharmaceuticals, which will increasingly rely on AI for R&D).

AI Ethics and Cultural Influence: AI systems encode the values and biases of their creators. If the creation of foundational AI models is concentrated in a few countries or companies, the ethical frameworks and cultural norms of those few will heavily influence AI behavior worldwide. This raises concerns about cultural hegemony. For instance, a conversational AI developed in California will have certain ingrained responses reflecting its training environment and the content filters its creators chose. Users in other cultural contexts might find those responses inappropriate or misaligned with local values, but without alternatives, they have little choice. Already, many foundation models “lack data representing diverse communities globally” (How to Make AI Equitable in the Global South), leading to blind spots and biases. A future where, say, three companies define the AI that billions of people use is one where a narrow set of ethical perspectives could dominate. This scenario makes it harder to ensure AI respects all societal needs and values. It also complicates global AI governance – whose ethical standards should prevail when an AI is used across borders? If non-Western nations have no seat at the table in AI development, their ability to influence these ethical settings is minimal. On top of that, closed control makes it harder for civil society and regulators globally to audit algorithms for issues like bias or misinformation, as discussed earlier.

These concerns have prompted calls for more distributed and inclusive approaches to AI governance. Some suggest that international collaborations on open models (like BLOOM) need to be encouraged to give more countries a stake in foundational AI technology. Others argue for regulations that require a degree of transparency or sharing of critical models, to prevent a few entities from hoarding all the power. Without such measures, the future could mirror and exacerbate existing inequities: leading AI powers will advance rapidly (and set the rules), while others become increasingly dependent, having to accept AI tools that might not fully serve their local interests (Global South's role in AI governance explored at IGF 2024). The AI divide would then deepen, with late-comers finding it nearly impossible to catch up.

In terms of global governance, if only a few corporations control AI, they might effectively become unelected policymakers. Their choices on AI behavior (what is considered “acceptable” output, how much privacy to give users, etc.) could become de facto standards worldwide. This elevates the importance of public sector oversight and open innovation. Many experts at forums like the IGF (Internet Governance Forum) and various UN groups emphasize that capacity-building in the Global South and open sharing of AI resources are key to a more balanced future (Global South's role in AI governance explored at IGF 2024). The alternative is a fragmented world where a few AI “gatekeepers” hold immense power over information and automation, which could have destabilizing effects on global economic and political structures.

In conclusion, partial openness in AI today has profound implications for who can benefit from the technology. True open-source AI – including open weights and data – could help level the playing field by allowing all communities to adapt and trust the tools to their needs. The current dominance of a few players makes it urgent to consider how we govern AI and ensure broader access. Open-source initiatives (Meta’s LLaMA, Stability’s models, BLOOM, etc.) are promising signs that alternatives to a locked-down AI ecosystem are possible. A global, inclusive approach to AI development and governance could spur innovation everywhere and mitigate power imbalances. Conversely, if we stay on a path where only a tight circle controls AI’s most powerful engines, global disparities in technology and influence will likely intensify, and the world’s diverse voices will struggle to be heard in the AI systems that increasingly shape our lives. The choices made now about openness, licensing, and collaboration will help determine whether AI becomes a tool of empowerment for all or primarily a source of power for the few.

Sources:

Open Source Initiative – “Meta’s LLaMA 2 license is not Open Source” (Meta's LLaMa 2 license is not Open Source) (Llama FAQs)
Red Hat – “Open source AI: Red Hat's point-of-view” (Open source AI: Red Hat's point-of-view)
Funnel Fiasco blog – “Open source AI must include open training data” (Open source AI and open data – Blog Fiasco)
NTIA (US) – Report on AI market concentration risks (Competition, Innovation, and Research)
Tony Blair Institute – “Risk of widening global AI divide” (China's AI Governance: Engaging the Global South)
CIGI / Issuu – “AI Innovation Concentration and the Governance Challenge” (AI Innovation Concentration and the Governance Challenge - Issuu)
World Economic Forum – “The AI divide between Global North and South” (The 'AI divide' between the Global North and Global South) ([PDF] responsible artificial intelligence efforts in the global south)
Stanford Social Innovation Review – “Making AI equitable in the Global South” (How to Make AI Equitable in the Global South)
Carnegie Endowment – “Contextualizing LLMs in Southeast Asia” (Contextualizing Large Language Models in Southeast Asia)
Internet Governance Forum 2024 – Discussion on Global South inclusion (Global South's role in AI governance explored at IGF 2024)
Bateman et al. – “Beyond Open vs. Closed: model weights and safety” (Beyond Open vs. Closed: Emerging Consensus and Key Questions ...)
ProPublica – Investigation of COMPAS algorithm bias (Racist Technology in Action: Predicting future criminals with a bias ...)
ResearchGate – “Without transparency, difficult to audit AI” (Ethical Implications of AI: Bias, Fairness, and Transparency)
Stability AI – Announcement of StableLM (open source LLM) (Stability AI launches StableLM, an open source ChatGPT alternative)
InfoQ – “Open-weight models enable independent deployment” (Open Weight Definition (OWD): Delivering Clarity while Protecting ...)
BigScience (Bloom) – “176B open-access multilingual model” (A 176B-Parameter Open-Access Multilingual Language Model - arXiv)
MVP.vc – “Open-source LLMs closing the performance gap” (Venture Bytes #110: Open Source LLMs Leveling the AI Playing Field)
Stanford HAI – “Movement to Decolonize AI (AI colonialism)”


Environmental Pressures of AI Infrastructure Beyond Electricity
Artificial intelligence requires a vast physical infrastructure (data centers, hardware, and supply chains) that carries significant environmental impacts beyond just electricity use. These impacts include heavy water usage for cooling, demand for raw materials and rare earth minerals (with associated mining concerns), growing volumes of electronic waste from rapid hardware turnover, and other resource strains like land use, manufacturing emissions, and supply chain challenges. Below, we explore each of these areas with a focus on known problem hotspots and backed by reports and studies.
Water Usage for Cooling AI Data Centers
AI supercomputing clusters and cloud data centers generate intense heat and often rely on water-cooled systems to keep equipment from overheating. This leads to enormous water consumption. On average, a typical data center uses around 300,000 gallons of water per day for cooling – roughly as much as 100,000 homes might use (The Environmental Costs of Generative Artificial Intelligence). Major “hyperscale” facilities (the kind used for AI and cloud computing) can draw 1 to 5 million gallons per day (The Environmental Costs of Generative Artificial Intelligence). For example, Google reported that in 2021 its data centers worldwide consumed about 4.3 billion gallons of water (about 450,000 gallons per data center per day on average) (Our commitment to climate-conscious data center cooling) (Google Developing New 'Climate Conscious' Cooling Tech to Save ...). This demand is rising fast: Microsoft disclosed that its water use jumped 34% in one year (2021–2022) to nearly 1.7 billion gallons annually, a surge attributed partly to its AI expansion (Artificial Intelligence a guzzler as Microsoft's water use rises 34%).

Such heavy water draw can strain local supplies, especially in water-scarce regions. A single large data center might guzzle up to 5 million gallons of drinking water a day, enough to supply tens of thousands of households or irrigate many acres of farmland (Data centers draining resources in water-stressed communities). In arid climates like the American Southwest, this has become a point of tension. For instance, in the desert state of Arizona, massive cloud and AI data centers (run by companies like Microsoft and Google) consume billions of gallons annually, raising concerns about depleting aquifers in an already drought-prone area (Data center water consumption is spiraling out of control - Reddit) (Data centers draining resources in water-stressed communities). Even in the cooler Pacific Northwest, conflicts have arisen – in The Dalles, Oregon, residents fought for transparency about Google’s data center water use after learning the facilities were drawing vast amounts from the local river system (The Dalles, Ore., Settles Lawsuit Over Google Data Centers) (Data center water consumption is spiraling out of control - Reddit).

Globally, the “thirst” of AI is drawing scrutiny. Uruguay, for example, experienced its worst drought in 70+ years and faced public outcry over Google’s plans to build a data center that would eventually require millions of liters of water per day from local supplies ('It's pillage': thirsty Uruguayans blast Google's plan to exploit water ...). Citizens protested what they saw as an attempt to exploit an already scarce resource during a water crisis. Similar worries have emerged in Chile, which has faced drought, and even in temperate countries like England, where utilities have considered restricting water use by data centers during dry spells (AI Is Taking Water From the Desert - The Atlantic) (Thames Water may restrict London data centers' water use - DCD). In sum, as AI data centers proliferate, they often compete with agriculture and communities for limited water. Each ChatGPT query or AI model training has an invisible water footprint – one analysis estimated that training OpenAI’s GPT-3 model (over a two-week period) consumed about 700,000 liters (~185,000 gallons) of fresh water for cooling in Microsoft’s U.S. data centers (ChatGPT would require 500 ml of water for every 20 to 50 questions ...). This illustrates how behind every impressive AI feat, there may be a very real impact on local water resources.
Material and Rare Earth Mineral Demand for AI Hardware
The hardware that enables AI – from high-end GPUs and specialized chips to servers and cooling systems – relies on numerous minerals and rare elements. Building these components requires critical materials that must be mined and processed, bringing environmental and ethical challenges at the source.

Key materials essential for AI hardware include:

Rare earth elements (REEs) – such as neodymium, dysprosium, and praseodymium, used to create the powerful permanent magnets in electric motors, disk drives, and cooling fans. These magnets are crucial for high-performance computing equipment (e.g. in disk actuators or cooling systems) due to their strength and heat resistance. However, rare earth mining is notoriously polluting and waste-intensive. Extracting REEs involves moving enormous amounts of ore and using harsh chemicals. For every ton of rare earth oxide produced, up to 2,000 tons of toxic waste are generated (Rare Earth Elements: A Global Scenario and Understanding of Their ...). This waste can include radioactive slurry and heavy metals that contaminate soil and water. Major rare earth mining regions (like Inner Mongolia in China, which dominates global REE supply) have suffered severe environmental damage from these processes (Rare Earth Elements: A Global Scenario and Understanding of Their ...).

Metals for Electronics – Copper, gold, and tin are found in circuit boards and chip interconnects; silicon is the base of all semiconductors. Mining and refining these common tech metals still entails habitat destruction and pollution (for example, gold mining can release cyanide and mercury). While not “rare,” their large-scale use in AI hardware means increased mining pressure globally.

“Tech Minerals” (Conflict and critical minerals) – Tantalum, tungsten, and others are used in capacitors and other components that go into GPUs and servers (Graphics Cards: A Lens on Global Manufacturing Inequality). These have been labeled conflict minerals when sourced from places like the Democratic Republic of Congo (DRC) where profits have fueled warfare. Tantalum (from coltan ore) and tungsten are often mined in unstable regions, raising concerns about human rights abuses and funding of armed conflict as well as environmental harm from unregulated mining.

Cobalt – a metal found in high-performance lithium-ion batteries and also used in some chip manufacturing processes. Data centers use batteries in backup power units, and AI hardware like robots or electric delivery infrastructure also use Li-ion batteries. Over half of the world’s cobalt comes from the DRC, where mining conditions are often appalling. There are well-documented child labor and worker safety issues in cobalt mines – reports describe children and adults in unsafe artisanal mines digging out cobalt by hand, often for meager pay (Combatting Child Labor in the Democratic Republic of the Congo's ...). This has made cobalt synonymous with ethical sourcing challenges. A U.S. congressional report noted that cobalt’s supply chain is “tainted” by child and forced labor, even as it remains critical for batteries and electronics (From Cobalt to Cars: How China Exploits Child and Forced Labor in ...). Environmentally, cobalt mining in the DRC has led to contaminated water and soil, as toxic byproducts are often not contained (The Environmental Impacts of Cobalt Mining in Congo | Earth.Org).

Lithium – another key battery material, used in backup power systems and energy storage that keep AI data centers online during power fluctuations. Lithium is primarily mined from brine under desert salt flats in places like Chile’s Atacama Desert and in parts of Argentina and Bolivia. These regions are very arid, and lithium extraction pumps huge volumes of groundwater into evaporation ponds. This can deplete water needed by local indigenous communities and wildlife. Studies in Chile have shown the Atacama salt flats are sinking and drying due to intensive lithium brine pumping (Lithium mining is slowly sinking Chile's Atacama salt flat, study shows). Massive quantities of fresh water – a “precious resource in these arid regions” – are diverted to lithium mining, contributing to habitat loss and conflicts with local water needs (The cost of green energy: lithium mining's impact on nature and ...).

Gallium and Germanium – lesser-known elements but crucial for semiconductors and fiber-optic communications (gallium is used in gallium nitride and gallium arsenide chips, germanium in fiber optics and as a semiconductor alloy). They are byproducts of other mining (often zinc and aluminum). Recently, their importance was highlighted by geopolitical moves: in mid-2023, China imposed export restrictions on gallium and germanium, since it is a leading producer of both (Is AI Driving Demand for Rare Earth Elements and Other Materials?). These curbs were seen as a response to chip technology tensions and have raised alarms about supply chain vulnerability for the West’s tech industry. It underlines that AI hardware is dependent on a fragile global supply of specialty materials, often controlled by only one or a few countries.

In summary, the demand for critical minerals is rising alongside AI’s growth. Each GPU, server, and data center requires a complex array of mined materials. The environmental toll of extracting these materials includes deforestation, enormous carbon emissions from mining operations, soil and water pollution from mine tailings, and often significant health hazards for miners and nearby communities. Ethically, the rush for materials like cobalt has been likened to past resource scrambles, with vulnerable populations bearing the brunt so that richer markets can build advanced tech (The Environmental Impacts of Cobalt Mining in Congo | Earth.Org). Organizations from the UN have pointed out that “microchips that power AI need rare earth elements, which are often mined in environmentally destructive ways” (AI has an environmental problem. Here's what the world can do ...) – a reminder that our digital innovations are tied to real-world extraction industries. Ensuring more responsible sourcing (such as recycling metals, fair labor practices, and stricter mining regulations) is a growing challenge as AI hardware deployment accelerates.
E-Waste and Hardware Turnover in the AI Era
AI systems rely on continual hardware upgrades to improve performance – newer chips, larger memory, more servers. This leads to a rapid turnover of equipment, which in turn creates electronic waste (e-waste) at an accelerating pace.

Frequent Upgrades: High-performance AI hardware often has a short useful life before it’s outclassed by newer technology. Data center operators commonly refresh servers every 3–5 years as a standard practice, and for AI GPU clusters the cycle can be even faster due to the quick progress in AI chip capabilities. Companies racing for AI superiority tend to replace chips and server units as soon as more efficient versions become available. These “rapid hardware refresh cycles” are fueling an e-waste surge in the tech industry (Green Materials Recent News | Data Center Knowledge). In other words, the push to constantly upgrade data center hardware to meet AI demands is leaving behind growing piles of outdated boards, chips, and devices.

Volume of E-Waste: Electronic waste is one of the fastest-growing waste streams globally. In 2022, an estimated 62 million metric tons of e-waste was generated worldwide (Electronic waste (e-waste) - World Health Organization (WHO)) – a figure that includes discarded computers, phones, servers, and other electronics. To put it in perspective, e-waste now weighs more than all commercial airliners ever made, annually. AI infrastructure is a contributing factor to this growth as cloud providers and tech companies decommission older servers and networking gear. Unfortunately, only a small fraction of electronics are properly recycled. According to the UN’s Global E-Waste Monitor, less than 20% of global e-waste is formally collected and recycled (Data Center Refurbishment: An Alternative Solution to Recycling). The rest often ends up in landfills, or is processed in unsafe informal recycling operations. This means most retired AI hardware could be adding to landfill waste, leaking toxins (like lead, mercury, and brominated plastics) into the environment, or being exported to developing countries where laborers recover scrap metal under hazardous conditions.

Disposal Challenges: Data center equipment can’t be simply tossed in the trash, but ensuring proper disposal isn’t easy. Often for security reasons, companies physically destroy storage devices and circuit boards to protect proprietary data (The Hidden Environmental Cost of Data Center Growth - Medium). This can hinder reuse: instead of refurbishing drives or servers for secondary markets, they may be shredded. While shredding allows materials to be recycled, it sometimes leads to mixed waste that isn’t efficiently recovered and thus still ends up in dumps. Additionally, specialized AI chips and hardware may lack well-established recycling streams due to their complex material composition.

Recycling and Reuse Efforts: There are initiatives to tackle data center e-waste. Some cloud companies have IT asset disposition (ITAD) programs to refurbish and resell used hardware. For instance, Amazon Web Services (AWS) reports that it sends all functional retired servers and components to reverse logistics centers to be sanitized, refurbished, or recycled (How AWS data centers reuse retired hardware - About Amazon). Google and others similarly claim high rates of component reuse and material recovery from their decommissioned equipment as part of sustainability goals. The European Union enforces e-waste directives (WEEE) requiring electronics recycling, pushing manufacturers to finance proper take-back of old equipment. Despite these efforts, the overall trend is concerning: the world’s e-waste is surging faster than our capacity to recycle, and valuable materials (like gold, copper, and rare earths in discarded electronics) are being lost. E-waste now constitutes about 70% of the toxic waste found in U.S. landfills, despite being only a small portion of total trash by volume (Apto Solutions Named A “Visionary” in Gartner ITAD Magic Quadrant). This highlights the toxicity of electronics (lead solder, cadmium, etc.) and the importance of improving the lifecycle of AI hardware. Extending hardware life (when possible), improving modularity for upgrades, and scaling up recycling systems will be key to preventing an AI-driven e-waste crisis.
Other Resource Uses and Supply Chain Impacts
Beyond power, water, and raw materials, AI infrastructure imposes several other strains on resources and the environment throughout its lifecycle and supply chain:

Land Use and Habitat Loss: Data centers require large tracts of land for their buildings, cooling infrastructure, and security buffers. The expansion of server farms can contribute to loss of farmland and natural habitat. For example, Northern Virginia (USA) – sometimes called “Data Center Alley” – has seen miles of farmland and forest cleared for data center development, encroaching on parks and rural communities (Bill of the Day: Data Centers' Impact on the Environment). Local residents have raised concerns about noise from massive cooling fans and diesel generators, light pollution from facilities running 24/7, and the alteration of scenic landscapes. In some cases, data centers are built in industrial parks, but in others they sprawl into formerly open land, affecting biodiversity. Construction of these facilities also requires energy-intensive materials like concrete and steel, adding to their environmental footprint. In short, the digital cloud still has a physical footprint: land use competition is an often overlooked issue, as regions attract data center projects for economic gain while grappling with the environmental trade-offs.

Emissions from Manufacturing and Construction: While running AI workloads consumes electricity (often the focus of carbon footprint discussions), there are significant embodied emissions in the hardware and infrastructure before they ever operate. Manufacturing chips and servers is a resource-intensive process. Fabricating advanced semiconductors involves complex chemical processes, high heat, and ultra-pure materials; this emits greenhouse gases and uses lots of energy and water. In fact, studies have shown that for many electronics, the CO₂ emitted during manufacturing can exceed the emissions from their electricity use over their lifetime (Hardware Efficiency | Learn Green Software). For example, producing a single high-end GPU involves mining and refining metals, producing silicon wafers, etching and packaging the chip – each step with a carbon cost. Similarly, building data center warehouses and equipping them (with cooling systems, backup generators, batteries, etc.) entails a large one-time carbon expenditure. These are classed as “Scope 3” emissions (indirect supply chain emissions) for companies. One notable concern is the use of HFC refrigerants in cooling systems (in chillers and air conditioners). Many data centers still rely on hydrofluorocarbon coolants that, if leaked, are extremely potent greenhouse gases. Although contained in closed systems, any leakage during operation or decommissioning adds to climate impact. Reducing reliance on HFCs (for example, using water or newer low-GWP refrigerants) is an emerging goal for greener data center operation (CCAC Webinar: Emissions from Cooling Systems in Data Centres).

Backup Power and Local Emissions: To ensure 24/7 uptime, data centers usually have diesel-powered generators for backup electricity during grid outages. These generators are rarely used at full capacity, but they must be tested regularly and kept ready. Diesel exhaust is highly polluting, containing carbon dioxide, nitrogen oxides, soot, and other particulates. In areas with many data centers, residents and environmental groups worry about cumulative air quality effects if generators run during emergencies or tests. It’s noted that “most data centers have chosen to rely on diesel generators for backup power, one of the most polluting forms of energy” (Data Centers, Diesel Generators and Air Quality – PEC Web Map). For instance, a single large facility’s backup generators can emit nitrogen oxide levels equivalent to thousands of idling cars when they run. Some operators are now exploring cleaner backup options (like fuel cells or battery banks), but diesel remains the industry standard, representing another environmental pressure point (especially if multiple centers cluster in one region).

Supply Chain Constraints and Geopolitical Impact: The AI boom has put unprecedented strain on global supply chains for high-tech components. Cutting-edge AI chips (GPUs, TPUs, etc.) rely on a handful of specialized manufacturing plants (fabs) and suppliers. During recent years, chip shortages revealed how stretched and delicate this supply chain is. Ramping up production to meet AI demand can lead to its own environmental issues. For example, new semiconductor fabs are being built in regions like Arizona and Texas in the U.S., which ironically are water-stressed areas – a new chip factory can require millions of gallons of water a day for production and cooling (Thirsty chip facilities under scrutiny in water stressed areas). TSMC’s planned Arizona plant is expected to consume 8.9 million gallons per day once fully operational (Thirsty chip facilities under scrutiny in water stressed areas), putting pressure on local water supplies even as the region copes with drought. This shows how scaling up the AI supply chain can transfer environmental burdens elsewhere (in this case, to the manufacturing stage and the communities hosting factories). Additionally, the intense demand for AI chips has led to supply bottlenecks, where companies scramble for limited components. This sometimes results in expedited shipping (e.g. flying hardware parts by air, which has a higher carbon footprint than sea freight) and potential relaxation of environmental standards to open new mines or factories quickly. Geopolitically, the concentration of critical mineral processing in a few countries means trade policies can have eco-consequences: when China restricted exports of gallium and germanium (needed for semiconductor and AI hardware production), other countries started exploring new mines and re-opening old facilities to compensate (Is AI Driving Demand for Rare Earth Elements and Other Materials?). Such rushes can lead to environmental oversight if done hastily. On the flip side, this situation is spurring interest in recycling and urban mining (recovering materials from old electronics) to reduce reliance on raw mining – a positive development if scaled up, since recycling metals generally has a lower environmental impact than mining virgin ore.

In summary, the environmental footprint of AI extends across its entire lifecycle. Beyond the widely discussed energy consumption and carbon emissions, AI infrastructure affects water resources, mining communities, waste streams, land use, and more. Each ChatGPT query or machine learning model rests on a vast industrial footprint: from the water-cooled servers in sprawling data centers, to the minerals dug out of the earth for chips, to the discarded hardware replaced by next year’s faster model. Recognizing these broader impacts is the first step toward mitigating them. Efforts such as improving cooling efficiency (to use less water), sourcing recycled materials for electronics, lengthening hardware refresh cycles, and developing greener supply chains will be crucial to make AI more sustainable. As one report put it, we must ensure the “digital revolution doesn’t become an environmental backslide”, by addressing these often hidden pressures alongside AI’s advancement (AI has an environmental problem. Here's what the world can do ...) (AI has an environmental problem. Here's what the world can do ...).


Centralized Control of AI: Recent Data and Analysis
Dominance of a Few Firms in AI Development
Concentration of Players: A handful of large organizations dominate advanced AI development. Companies like OpenAI (closely partnered with Microsoft), Google (Google Brain/DeepMind), Meta (Facebook), Microsoft itself, Amazon, and leading Chinese firms (Baidu, Alibaba, Tencent) are at the forefront. A 2023 UK government study found that a small group of top AI firms (e.g. Microsoft, Google, Meta, OpenAI, Anthropic) accounted for about 20% of AI sector employment and nearly 40% of revenues and value-added (Artificial Intelligence sector study 2023 - GOV.UK) (Artificial Intelligence sector study 2023 - GOV.UK), underscoring the economic concentration in this field.

Industry Outpacing Academia: Cutting-edge AI research is increasingly centered in industry labs rather than universities. In 2022, industry produced 32 significant new machine-learning models, vs. just 3 from academia ([PDF] Artificial Intelligence Index Report 2023 - Stanford University). The trend accelerated in 2023, when companies built 51 notable AI models compared to 15 by academia (AI Index Report 2024 – Artificial Intelligence Index - Stanford University). A majority of top AI talent now works in industry; for example, 65% of AI PhD graduates in the U.S. took jobs in industry (vs ~28% in academia) (AI Index Report 2023 – Artificial Intelligence Index - Stanford University). These indicators show how corporate labs – primarily Big Tech companies – have become the main drivers of AI advancement.

Massive Investments: The leading firms invest billions of dollars in AI, creating a high barrier to entry for others. For instance, Alphabet (Google’s parent) spent around $1.5 billion on its AI subsidiary DeepMind in 2019 (Sharing: The Growing Influence of Industry in AI Research), just one part of its overall AI spending. Such investment gives big players a huge edge in developing “foundation models” (very large AI models) that smaller companies or academics cannot easily replicate. In China, tech giants similarly pour resources into AI (often with state support), meaning most AI R&D globally is concentrated in a few Western and Chinese corporations. This consolidation raises concerns that no more than a dozen organizations globally set the agenda for how AI evolves.
Control of Hardware, Data, and Software Infrastructure
Dominant AI firms reinforce their leadership by controlling vital hardware, data pipelines, and software that AI development depends on:

Hardware (Compute Power): Advanced AI requires enormous computing resources, and access is largely controlled by a few companies. NVIDIA – the leading AI chipmaker – holds about 92% of the market for AI GPUs as of early 2024 (AI Chipset Sales Are About To Blast Off - Techstrong.ai), creating an effective monopoly in AI hardware. Meanwhile, cloud computing is dominated by Amazon, Microsoft, and Google, which combined control roughly 68% of global cloud infrastructure services (Cloud Market Share For $84B Q3 2024: AWS, Microsoft, Google ...). This means most AI startups or researchers must rely on these companies’ hardware or cloud platforms to train and deploy large models. The U.S. FTC has noted that exclusive partnerships between cloud providers and AI labs could further “affect access to key components” like compute power and even talent for other competitors ([PDF] FTC Report On AI Sector Illuminates Future Enforcement).

Data Pipelines: Access to large-scale data is another chokepoint. The leading firms leverage massive data pipelines — from web crawls to private user data. For example, Google’s dominance in search (around 90% global search market share (Search Engine Market Share in 2024 [Dec '24 Update] - Oberlo)) gives it unparalleled access to internet data for training AI. Meta can tap into billions of social media posts from Facebook/Instagram, and Microsoft/OpenAI have reportedly used internet-scale datasets (GitHub code, Common Crawl web data, etc.) for training. These companies’ data advantages are hard to match. Much of the training data for AI is in English or sourced from Western-centric websites, reflecting the origin of these pipelines. Efforts to obtain diverse or proprietary data (for medical AI, finance, etc.) also often involve partnerships with the same big firms. Control of data thus reinforces their AI leadership.

Software and Platforms: The software ecosystem for AI — frameworks, libraries, and pretrained models — has been heavily shaped by big tech. Google developed TensorFlow and OpenAI/Meta drove PyTorch (now the most popular deep learning library). While these tools are open source, their development is steered by corporate priorities. Crucially, many foundation models are kept proprietary. OpenAI’s GPT-4, for example, was released in 2023 with no details on its architecture or training, a move widely criticized as “not transparent or ‘open’ in any meaningful way” (Why OpenAI Needs To Be Singled Out in the Troubled Tech Valley). Only the big companies know how these models are built or decide how they evolve. An October 2023 Stanford Foundation Model Transparency Index evaluated ten major AI developers and found them lacking in transparency about how their models are created and operate (New index rates transparency of ten foundation model companies ...). Even when companies do open-source a model (such as Meta releasing LLaMA 2 in 2023), the decision is top-down and strategic. Overall, the dominant firms’ control of core software and model IP means the broader community often relies on what those few entities choose to release.

Impacts on Transparency and Community Input: Because of the control of these key resources, transparency suffers. Outside researchers and the public get only limited insight into the “black box” of big models. For instance, OpenAI now discloses far less about its models than it did in its early days (Why OpenAI Needs To Be Singled Out in the Troubled Tech Valley). Moreover, community input into AI development is minimal – feature roadmaps and training choices are set internally by companies. Some developers have even called for OpenAI and others to be more open in communicating model updates and involving the community. (In one forum, users appealed for “inclusive, transparent” governance of AI systems instead of closed decision-making (Call to action: Community takeover, Your Voice Matters).) By controlling hardware, data, and software, the tech giants can decide unilaterally what features to prioritize, which safety measures to implement, and when to deploy updates – often revealing changes only after the fact. This top-down control leaves smaller stakeholders (users, independent developers, affected communities) with little say.
How AI Features and Updates Are Decided
Opaque Decision-Making: Major AI firms determine feature priorities and model updates largely behind closed doors. The process is typically driven by internal product strategy, competition, and risk management, rather than by public input. For example, OpenAI’s decisions on model improvements or API changes are not made transparently – users often only learn about new features (or restrictions) at launch. There is little external visibility into why certain capabilities are released or held back. When GPT-4 was introduced in 2023, OpenAI provided only a barebones technical report and declined to reveal model size or training details, citing competitive and safety reasons (Why OpenAI Needs To Be Singled Out in the Troubled Tech Valley). Similarly, Google’s AI updates (like new features in Bard or improvements to its Translate and Vision AI) are announced in polished terms but without disclosing much about whose needs were considered in prioritization.

Lack of Transparency and Oversight: This closed development model means that the criteria for prioritizing features are not subject to public scrutiny. Companies may prioritize features that align with business interests or mainstream user bases – for instance, integrating AI assistants into profitable products or optimizing models for languages with large markets – while niche or community-requested features get less attention. There is no formal mechanism for diverse communities to influence these priorities. Unlike open-source projects where roadmaps can be discussed openly, corporate AI product roadmaps are proprietary. Even developers who rely on these AI APIs have expressed frustration at the unpredictability; some have asked for clearer timelines on new features and more communication from providers (Can we get more transparency/timelines on API new features).

Community Input and Governance: So far, community or stakeholder input in big AI systems’ development has been limited. AI companies do conduct user studies and gather feedback (for example, OpenAI used human feedback to fine-tune ChatGPT’s behavior), but the governance of these systems is not community-driven. Policy decisions — such as what content an AI model will refuse to produce, or which languages to support — are made internally. There have been calls for more inclusive approaches; one group of AI developers even proposed a “community takeover” model of governance that would be more democratic and transparent (Call to action: Community takeover, Your Voice Matters). However, as of now, dominant firms retain centralized control over what their AI products do and whose needs get prioritized. This opacity can erode trust and makes it hard for outsiders to understand or challenge the direction of AI technology.
Cultural Diversity vs. Western-Centric AI Development
Western-Centric Development: Modern AI development largely reflects Western corporate interests and perspectives. The leading AI firms are U.S.-based (or U.S.-aligned), and they naturally focus on Western markets first. As a result, AI models often skew toward English-language and Western culture in their training data and use cases. In 2023, the United States far outpaced other regions in producing state-of-the-art AI models – 61 notable models came from the U.S., compared to 15 from China and 21 from the entire EU (Stanford just released its annual AI Index report. Here's what it reveals). This imbalance means Western viewpoints and values heavily influence AI systems. A report by researchers notes that because AI development is largely Western-led, there is a need for a global, multicultural perspective to address biases (Global perspectives on AI bias: Addressing cultural asymmetries ...) – implying that such perspective is currently lacking.

Underrepresentation of Other Cultures: Studies are finding that generative AI systems can misrepresent or marginalize non-Western cultures. For instance, research from Penn State and the University of Washington in 2023 found “non-Western cultures [are] misrepresented [or] harmed by generative AI” outputs (Non-Western cultures misrepresented, harmed by generative AI ...). This can manifest as bias in image generators (e.g. under-representing certain ethnic attire or cultural settings) or in language models that perform poorly in less-common languages and dialects. Many AI models have been trained predominantly on English data (or Chinese, for China’s models), with far less data from African, South Asian, Latin American, or indigenous sources. Thus, the default behavior of AI often aligns with the cultural context of its creators (e.g., answering questions with assumptions suitable for American users). There is also the risk of AI content moderation enforcing Western norms globally – for example, what U.S. companies deem “appropriate” speech could conflict with local cultural norms elsewhere.

Efforts to Broaden Inclusivity: There are initiatives aiming to make AI more globally inclusive. One notable example is the BigScience BLOOM model (2022), a project by an international team of researchers. BLOOM was trained on data in 46 languages (including many from Africa, Asia, and South America) and was openly released to encourage wider use (BigScience Releases 176B Parameter AI Language Model BLOOM). Similarly, Meta’s open-release of Llama 2 in 2023, while not as multilingual as BLOOM, at least provided a free model that anyone (across different countries) could adapt to their needs. Despite these efforts, the overall direction of AI still primarily follows Western corporate priorities – typically, optimizing for major global languages, and for applications that serve large markets or advertisers. Culturally specific needs (like local dialect understanding, or region-specific ethical considerations) may get less attention unless they align with the strategy of a dominant firm or a well-resourced government. Going forward, many experts argue that involving stakeholders from different cultures in AI design and governance is crucial to avoid a one-size-fits-all, Western-biased AI landscape (Global perspectives on AI bias: Addressing cultural asymmetries ...).
Global AI Governance: U.S., China, EU, and International Efforts
Government policies and international frameworks are now emerging to address the dominance and impact of AI, with different approaches in the U.S., Europe, China, and beyond:

United States: The U.S. has so far taken a light regulatory touch at the federal level, relying on existing laws and agencies to oversee AI. There is no single AI law yet, but regulators are increasingly scrutinizing Big Tech’s AI power. In 2023, the Federal Trade Commission opened inquiries into AI industry practices – for example, studying cloud provider partnerships with AI startups (like the exclusivity between Microsoft Azure and OpenAI) which might raise competition concerns ([PDF] FTC Report On AI Sector Illuminates Future Enforcement). The U.S. has also issued guidance on AI ethics: the White House OSTP released a “Blueprint for an AI Bill of Rights” (late 2022) and in October 2023 the Biden Administration issued an executive order on “Safe, Secure, and Trustworthy AI.” That executive order laid out measures like requiring companies to share the results of safety tests for highly capable AI models with the government and tasked NIST with developing AI safety standards (A Closer Look at the Executive Order on the Safe, Secure ... - NITAAC). Additionally, the U.S. government convened leading AI firms to pledge voluntary commitments (such as external red-teaming of models and watermarking AI-generated content). These steps indicate the U.S. is seeking to manage AI risks (security, fairness) without stifling innovation – though critics say more binding rules may be needed.

European Union: The EU has taken a more assertive regulatory stance. It is finalizing the EU AI Act, set to be the world’s first comprehensive legal framework on AI (European approach to artificial intelligence). This legislation uses a risk-based approach: it will ban certain harmful AI practices outright (like social scoring), impose strict requirements on “high-risk” AI systems (e.g. in healthcare, recruitment, policing), and introduce transparency obligations for generative AI and large models. For example, providers of foundation models may have to disclose information about their training data and put in place risk controls under the AI Act’s provisions. The EU’s goal is to address AI risks and ensure accountability, while positioning Europe as a global leader in trustworthy AI governance (European approach to artificial intelligence). In addition to the AI Act, EU regulators (such as competition and data protection authorities) are actively watching AI dominance – the EU has already enforced GDPR privacy rules on AI products (Italy’s privacy authority famously banned ChatGPT for a month in 2023 over data violations, prompting fixes) (Italy Fines OpenAI €15 Million for ChatGPT GDPR Data Privacy ...). Overall, Europe is injecting considerations of fundamental rights, transparency, and pluralism into the AI development equation, in contrast to the more market-driven U.S. approach.

China: China’s government is pursuing a strategy of state-guided AI development with heavy regulatory oversight. On one hand, China invests heavily in AI research and supports national champion companies (like Baidu’s AI labs, Alibaba Cloud, Huawei’s AI chip research) to compete with U.S. tech. On the other hand, Beijing has moved quickly to regulate the AI ecosystem, especially regarding content and security. In 2023, China became the first country to propose and implement comprehensive generative AI rules ([PDF] The Promise and Perils of China's Regulation of Artificial Intelligence). The Cyberspace Administration of China (CAC), along with other agencies (MIIT, etc.), issued the Interim Measures for Generative AI Services, effective mid-August 2023. These rules require any AI service offered to the public in China to register with authorities and undergo security assessments (Spotlight Series on Global AI Policy — Part III: China's Policy ...). Providers must ensure AI content aligns with “core socialist values” and does not produce harmful or politically sensitive material. There are also mandates to label AI-generated content and to respect data provenance/copyright in training data. This means AI development in China operates under strict government content filters and licensing – essentially a centralized governance model. Culturally and politically, China’s AI reflects government priorities (e.g., strong censorship of undesired outputs, emphasis on applications like surveillance). While the Chinese approach is very different from the West’s, it similarly concentrates decision-making (in this case, in the hands of the state and a few large companies it oversees).

International and Multilateral Efforts: Recognizing that AI’s challenges are global, there have been moves toward international AI governance. The United Nations has urged a coordinated approach: UNESCO’s Recommendation on the Ethics of AI (adopted by nearly 200 countries in 2021) established broad ethical guidelines, and in 2023 the UN Secretary-General proposed an international AI advisory body. The G7 nations launched the “Hiroshima AI Process” in mid-2023 to harmonize principles for “trustworthy AI” among advanced economies, and discussed a voluntary Code of Conduct for AI firms. Likewise, the OECD’s AI Policy Observatory is facilitating research and guidelines on AI policy (building on the OECD’s AI Principles which the G20 endorsed). These international efforts are still nascent but aim to prevent a vacuum where only a few corporations or countries set the rules. Global AI governance seeks to increase transparency, promote safety standards, and ensure AI aligns with human rights across borders. However, the effectiveness of these efforts will depend on cooperation – for example, whether the U.S. and China can agree on any common norms, and whether companies will abide by globally negotiated standards. So far, much of global governance is soft law or guidance, while hard regulations remain at the national/regional level (EU, China, etc.).

In summary, global regulators are beginning to respond to the concentration of AI power: the U.S. through targeted inquiries and guidelines, the EU via broad legislation (the AI Act) enforcing transparency and accountability, China via state control and strict rules, and international bodies via collaborative frameworks. These measures, if effectively implemented, could check some negative effects of AI monopolization – for instance, by compelling greater transparency or preventing abusive practices – but they also must balance not hindering innovation.
Effects of AI Monopolization on Innovation, Privacy, and Accessibility
Innovation and Competition: There is growing concern that AI’s monopolization by a few big players could stifle innovation in the long run. When a market is highly concentrated, dominant firms can potentially shape the direction of research and deployment in ways that serve their interests, possibly at the expense of breakthrough innovations that might have come from smaller entrants. Experts note that market concentration tends to undermine innovation and harm consumers (Monopoly Power Is the Elephant in the Room in the AI Debate). If only a few companies control the most powerful models, they might limit access or favor their own services (so-called “self-preferencing”), making it hard for startups or academia to compete or to explore alternative approaches. On the other hand, it’s also true that these tech giants are pouring huge resources into pushing AI forward – so we are seeing rapid innovation, but within a paradigm set by those firms. The risk is that diversity of ideas suffers: many AI researchers now work for the same handful of employers, and independent or open-source AI projects struggle for computing resources. This could lead to a less competitive ecosystem over time. Antitrust regulators are watching these dynamics; for example, the FTC’s recent study of cloud–AI partnerships suggested that certain exclusive deals could limit other firms’ ability to get critical AI resources ([PDF] FTC Report On AI Sector Illuminates Future Enforcement). Ensuring healthy competition (through open-source efforts, funding for academic research, or antitrust action if needed) may be important to keep innovation broad-based and not just incremental improvements by incumbents.

Privacy and Data Use: AI monopolization also raises serious privacy issues. The dominant AI companies control astonishing amounts of data – including personal data – which they use to train and refine models. Users often have little choice but to trust these few providers with their information (for instance, every prompt you enter into ChatGPT is data that OpenAI/Microsoft could analyze). This concentration increases the impact of any privacy breach or misuse. A single company’s policies on data can affect millions. We’ve already seen incidents that set off privacy alarms: in March 2023, Italy’s data protection authority temporarily banned OpenAI’s ChatGPT after a data leak and alleged GDPR violations, citing concerns about how user data was being collected and used (Italy Fines OpenAI €15 Million for ChatGPT GDPR Data Privacy ...). OpenAI had to hastily implement age checks and more transparency to get the ban lifted, and eventually was fined €15 million for privacy breaches (Italy fines OpenAI over ChatGPT privacy rules breach - Reuters). This episode highlights that when one of the few AI providers missteps, it can have wide repercussions. Moreover, the training datasets for big AI models often include scraped content from the internet, potentially including personal information obtained without consent. Concentrated control means these practices are decided by a few entities rather than through a public process. Privacy advocates worry that without competition, users lack alternatives that might offer better privacy. Stronger privacy regulation (like Europe’s GDPR) and greater transparency from AI firms about data handling are seen as necessary checks. Otherwise, a monopolistic AI landscape could normalize extensive surveillance and data mining as the price for AI services.

Accessibility and Equity: When cutting-edge AI is controlled by a few large companies, access to AI capabilities can be unequal. These firms may charge high fees for API access or prioritize serving lucrative markets, which can put advanced AI out of reach for smaller businesses, researchers, or under-resourced communities. For example, organizations that cannot afford OpenAI’s fees or an Azure cloud contract might be left behind in implementing AI solutions. This could widen the gap between big tech-enabled companies and everyone else. There’s also the aspect of technical accessibility: if model weights and code aren’t open, users are forced to rely on black-box systems via limited interfaces, which hinders the ability to understand or improve them. The community of AI practitioners outside Big Tech often has to work with less powerful open models. Encouragingly, the past two years have seen a surge in open-source AI models and tools, which push back against monopolization. The release of Stable Diffusion (image generation) and Meta’s LLaMA family, and projects like BLOOM, are democratizing access to AI. Some analysts argue that these open-source efforts will ensure the AI revolution “will not be monopolized” – i.e. that no single entity can completely dominate, because open alternatives keep emerging (The AI Revolution Will Not Be Monopolized - InfoQ). Indeed, when Meta released LLaMA 2 as open source, it explicitly cited the importance of community contributions and broad access. However, even these open models often rely indirectly on resources from the big players (for instance, many were trained on cloud platforms provided by those firms, or with their funding). The accessibility issue also extends globally: communities in the Global South worry about having to depend on Western AI APIs that may not cater to local languages or needs. Monopolization could thus exacerbate global digital inequalities if not addressed. Ensuring accessible AI might involve open-source development, public-sector support for AI research, and cloud compute subsidies or infrastructure for researchers – otherwise, AI’s benefits might remain locked behind the gates of a few corporations.

In conclusion, the centralization of AI development in a few large organizations has far-reaching implications. It has enabled astonishing progress in AI thanks to these firms’ resources, but it also concentrates power over technology that affects everyone. Transparency and public accountability have suffered as competitive secrecy and proprietary control become the norm. Many cultural and community perspectives are not adequately represented in current AI systems, since development is driven by a relatively narrow group of players. Meanwhile, policymakers around the world are waking up to the need for AI governance – from the EU’s sweeping regulations to China’s state-controlled approach – in order to check corporate dominance and align AI with societal values. Going forward, balancing innovation with oversight will be key. Opening up aspects of AI (through open models or research partnerships) and instituting global norms for safety, fairness, and openness could help counter the downsides of monopolization. The period of 2023–2024 has shown both the incredible power of centralized AI (e.g. ChatGPT’s worldwide impact) and the urgent calls from researchers, governments, and citizens to ensure this power is used transparently and inclusively, rather than solely by and for the tech giants that currently lead the field.  (Monopoly Power Is the Elephant in the Room in the AI Debate) (Call to action: Community takeover, Your Voice Matters)

You are in a debate and the issue being debated is "the positive benefits of AI are greater than the negative tradeoffs for society". you are arguing against this point. present your position in a minimum of 1000 words. Your position must cite data and other relevant information from the research. Anticipate the other side's case and incorporate rebutting information

#### ChatGPT:
<time datetime="2025-02-14T05:35:00.165Z" title="2/14/2025, 2:35:00 PM">14:35</time>

Artificial intelligence (AI) has risen to prominence in recent years with promises of revolutionizing everything from manufacturing to medicine. Indeed, proponents claim AI can enhance productivity, personalize learning, accelerate scientific discovery, and even address climate change through better modeling and optimization. Yet beneath these grand promises lies a far more complicated reality. The substantial risks and negative tradeoffs of AI too often go under-acknowledged, ranging from environmental harms and personal data misuse to bias, concentration of power, addictive design, and the erosion of user autonomy. Taken in totality, these drawbacks suggest that the supposed “positive benefits” of AI are not, at this time, clearly outweighing the societal costs. Below, I will lay out the reasons why we should be deeply skeptical of the idea that AI’s benefits automatically surpass its harms, drawing upon recent data and research findings, while also addressing some of the common points of the pro-AI argument to illustrate why they are incomplete.

---

### 1. Environmental Footprint: Energy, Water, and Material Extraction

One of the central overlooked costs of AI is its growing environmental impact. Proponents often claim that AI can reduce carbon footprints by optimizing energy grids or improving logistics. While there is some truth in these potential efficiencies, the real-world data from recent years shows that AI’s own energy appetite is growing at an alarming pace. Data centers worldwide consumed roughly **460 terawatt-hours (TWh)** of electricity in 2022—about **2% of global electricity usage**. Although AI workloads were a fraction of that total in 2022, they have been expanding rapidly as the popularity of large language models (LLMs) surges. According to one study, AI-specific workloads used **5–9 TWh** in 2023, and forecasts suggest this could rise to hundreds of TWh by the end of the decade.

Moreover, electricity usage is only part of the environmental equation. AI data centers are voracious consumers of water for cooling: a single large facility can require **1 to 5 million gallons of water per day**. Microsoft’s water usage jumped **34%** between 2021 and 2022, reaching nearly 1.7 billion gallons, a surge largely attributed to its AI expansion. In desert regions like Arizona, local residents and environmentalists have raised the alarm about data centers depleting aquifers and straining municipal water supplies. Uruguay faced public outcry after Google proposed building a data center that would potentially withdraw millions of liters of water daily during a major drought. The water footprint of AI thus places it in direct competition with agriculture and communities for this finite resource.

On top of energy and water, AI hardware relies on rare earth minerals and other critical resources. High-performance servers and GPUs contain materials like **cobalt**, **lithium**, and **rare earth elements** mined under often severe environmental and labor conditions—particularly in developing nations. Mining cobalt, for example, has been documented to involve child labor in the Democratic Republic of Congo, while lithium extraction in Chile’s Atacama Desert depletes fragile water reserves. Each new wave of AI hardware churns out **e-waste** at a faster pace too: servers and GPUs are replaced every few years to maintain cutting-edge performance, feeding an already growing stream of discarded electronics. Proponents might point to small-scale recycling initiatives and greener data center designs, but these do not negate the reality that AI’s resource demands are rising exponentially, with significant ecological and human costs.

---

### 2. Privacy Violations and Surveillance

Advocates often celebrate AI’s capacity to offer personalized recommendations or advanced analytics, yet these functionalities hinge upon mass data collection and surveillance. OpenAI’s ChatGPT, Google’s Bard, Meta’s platforms, and many others gather user interactions—prompts, browsing histories, voice recordings—on an unprecedented scale. Even if stored data is “anonymized,” it remains prone to misuse or re-identification. In 2023, Italy’s data protection authority temporarily banned ChatGPT over concerns regarding GDPR violations, illustrating how quickly AI companies can overreach in data handling.

Moreover, the concentration of personal data in the hands of a few major AI providers raises concerns about potential government surveillance or corporate misuse. A single entity with records of billions of daily user interactions wields enormous power to shape discourse, influence behavior, or sell advertising. The “positive” side of AI might say that advanced models could improve user experience or safety. But as the Federal Trade Commission (FTC) has noted, these data-centric business models enable new forms of profiling and manipulation that can undermine personal autonomy—far from an unequivocally positive benefit.

---

### 3. Manipulative Design and Mental Health Concerns

A common refrain from AI proponents is that AI-driven social media and recommendation systems improve content discovery and user convenience. In reality, these same systems are strategically engineered to maximize user time-on-platform. Developers leverage reinforcement learning and advanced recommendation algorithms to exploit psychological vulnerabilities—such as intermittent rewards, FOMO (fear of missing out), and social validation feedback loops.

Empirical studies have linked heavy social media usage with increased rates of depression, anxiety, and loneliness. The University of Pennsylvania found that limiting social media use to roughly 30 minutes per day significantly reduced reported loneliness and depression symptoms compared to typical high usage. Platforms like TikTok, YouTube, and Instagram employ AI to serve endless feeds that trap users. This might generate “engagement” and advertising revenue, but it can also harm mental well-being, degrade attention spans, and undermine genuine social connections. Yes, AI might deliver amusing content or connect users worldwide, but the manipulative design integral to many AI-driven platforms presents a serious negative tradeoff that must not be trivialized.

---

### 4. Bias, Cultural Erosion, and Limited Global Representation

Although supporters of AI champion it as a “universal tool,” in practice it remains heavily shaped by Western-centric data. Mainstream LLMs like GPT-4, Bard, and LLaMA primarily reflect English language resources and cultural assumptions. The potential for local or marginalized communities to be systematically misrepresented is substantial. Research has shown that generative AI models often misrepresent non-Western traditions or fail to handle their languages competently. When these systems become integrated into health care, legal advice, or education, they risk embedding cultural biases that can perpetuate stereotypes or deliver sub-par services to historically underserved groups.

While some initiatives—such as BigScience’s BLOOM or meta’s multilingual expansions—aim to broaden language coverage, these remain exceptions, not the rule. The majority of mainstream AI development revolves around markets where data is easiest to obtain and monetization is straightforward. This dynamic can further cement global cultural hegemony, where local knowledge, values, and nuances are overshadowed by Western technology norms. Any honest calculation of “net benefits” from AI has to grapple with the question: who benefits, and who is left behind?

---

### 5. Centralized Corporate Control and Reduced Competition

Proponents tend to depict AI as a rising tide that lifts all boats. The reality is that a handful of corporations—Google, Microsoft, Meta, Amazon, as well as Chinese giants like Baidu and Alibaba—dominate the advanced AI landscape. According to a 2023 UK government study, a small set of top AI firms accounted for nearly **40%** of the sector’s revenues and value-added, while manufacturing the crucial hardware also rests in the hands of a near-monopoly (NVIDIA controlling around **92%** of the AI GPU market).

This consolidation potentially undermines innovation and competition. Smaller developers or researchers cannot match the billions of dollars and the massive data pipelines needed to train cutting-edge models. As a result, these large entities have near-complete authority to set AI development agendas—determining which languages are supported, what types of content are censored, or how user data is handled. Even well-intentioned or academically oriented projects frequently rely on Big Tech’s cloud infrastructure to carry out large-scale training.

The pro-AI argument often emphasizes how quickly these large firms push forward new features. However, “innovation” that is guided solely by corporate interests is not guaranteed to serve the public good. Centralized control also facilitates the creation of black-box algorithms, hampering transparency, external auditing, and accountability. In effect, a society that depends on a tiny number of companies for critical AI services may find that it lacks real choices about how AI is deployed.

---

### 6. Challenges in Governance and Regulation

We hear from the pro-AI side that global regulators—particularly in the EU, U.S., and China—are stepping in to ensure responsible AI use. While the EU’s AI Act and China’s generative AI regulations aim to impose rules on transparency and high-risk applications, implementation remains in its infancy. In the United States, the federal government has taken only preliminary steps (like the “Blueprint for an AI Bill of Rights” and a 2023 executive order on AI safety). One might think that regulation alone will ensure the negative consequences do not outweigh the positives, but the complexity of AI’s global supply chains and the rapid pace of innovation complicate any oversight. Even highly capable authorities struggle to adequately monitor the training data and operational details of these large models. Some of these new regulations focus on content moderation or risk classification but do less to address the underlying concentration of power, environmental resource strain, or manipulative engagement strategies.

Additionally, in authoritarian contexts or for military uses, AI can supercharge surveillance and oppression. China’s advanced facial recognition and social credit systems provide cautionary tales of how AI can become an enabler of mass surveillance, censorship, and social control. Even in democratic nations, law enforcement agencies are adopting AI tools—like predictive policing or facial recognition—often with minimal transparency or public input. None of this aligns with the optimistic claim that AI solely delivers positive benefits. If anything, it underscores how quickly AI can morph into a tool for authoritarian control or invasive policing, absent strong and enforceable safeguards.

---

### 7. Anticipating Pro-AI Arguments—and Why They Fall Short

**“AI Will Solve Major Crises, Like Climate Change.”**\
While AI can certainly optimize energy usage or improve supply chain management, the actual net carbon footprint of AI is non-trivial, as it requires massive data centers running on grids still largely powered by fossil fuels. Furthermore, building new server farms accelerates the consumption of limited fresh water and resources. Any emission savings from AI-driven efficiency must be balanced against these direct impacts. As documented, training a single large language model can consume hundreds of thousands of liters of water; Google’s data centers collectively used about 4.3 billion gallons of water in 2021. It is an oversimplification to treat AI as a purely green solution.

**“It Enhances Human Productivity and Spawns New Jobs.”**\
Indeed, AI can automate repetitive tasks and open new roles in data science or AI ethics. However, at scale, many jobs involving routine cognitive or even creative work may be displaced with fewer new roles than replaced ones, especially in smaller markets or lower-skilled positions. The net effect on labor markets and wages remains uncertain. More importantly, if only a few large players reap the bulk of the productivity gains, then inequality may worsen. Gains are not automatically shared with workers or smaller businesses.

**“AI Expands Accessibility and Education Worldwide.”**\
Remote learning tools and assistive AI-based applications can be beneficial in principle. But these tools often require robust internet infrastructure, English-language content, and the financial means to pay for advanced AI subscription services. Areas lacking reliable connectivity, or communities needing localized language support, remain under-served. True accessibility demands open models, open data, and inclusive development that so far remain the exception rather than the norm.

**“Regulations Will Keep AI Safe and Ethical.”**\
While it is encouraging that governments and organizations are trying to set ethical standards, current guidelines are mostly voluntary or focus on narrow aspects like transparency or content moderation. Even as the EU’s AI Act and China’s generative AI rules go into effect, they encounter formidable challenges in enforcement and potential political interests overshadowing public well-being. Privacy fiascos and ethically questionable deployments are still emerging, indicating that regulation alone lags behind real-world deployment.

---

### Conclusion: Negative Tradeoffs Are Substantial and Underrecognized

The idea that AI’s advantages automatically eclipse its disadvantages downplays the immediate and long-term ramifications that are already visible. AI is, indisputably, driving transformations, yet it leaves in its wake problems that are large-scale, concrete, and often global in scope. We see **escalating energy usage** (with forecasts showing AI data centers potentially doubling global data center electricity consumption by 2026), **massive water consumption**, significant **e-waste and resource extraction impacts**, and a **centralization of digital power** that can subvert democracy and overshadow local cultures. We also see how AI-based systems exploit user psychology to maximize profit, sometimes at the expense of mental health.

Equally concerning is the lack of democratic governance or transparent processes guiding how AI evolves. A tiny number of corporations and governments shape AI’s direction behind closed doors. Their decisions can amplify biases, degrade privacy, or concentrate economic benefits among a select few. Meanwhile, the global majority, especially in the Global South, may face barriers to customizing AI for their own contexts or see their data appropriated without consent.

Certainly, beneficial uses exist—such as medical image analysis, language translation, and more efficient resource management—but it is overly simplistic to claim that these positives “outweigh” the negatives when we consider environmental damage, social fragmentation, addictive design, job displacement, ethical concerns, and oligopolistic control. Achieving a genuinely positive balance would require a fundamental shift toward open-source models, robust international regulations, stronger data protections, environmental accountability, and inclusive governance structures that allow communities worldwide a seat at the table. Until such systemic reforms are implemented and proven effective, we have every reason to doubt that the benefits of AI overshadow its mounting negative tradeoffs.


#### You:
<time datetime="2025-02-14T15:27:49.022Z" title="2/15/2025, 12:27:49 AM">00:27</time>

excellent content. now edit his position using the yale style guide posted below. there is no limit on length for this edit.
```
WRITERS.COM
CONCISE WRITING: HOW TO OMIT
NEEDLESS WORDS
Sean Glatch | December 2, 2022
This article is all about concise writing, summed up in the timeless phrase
“Omit needless words.” We’ll examine some of the best advice on writing
concisely, define what concise writing is and isn’t, and describe 11 writing
habits that encourage concision.
“OMIT NEEDLESS WORDS” AND The
Elements of Style
In 1920, William Strunk Jr. published The Elements of Style. This was a
groundbreaking work for writers, as it was the first English style guide—
emphasizing, among other things, the importance of concise writing.
Since then, Strunk’s style guide has been adapted and edited several times; in
1959, E. B. White doubled the book’s length with his own advice.
The Elements of Style has this to say about concision:
Omit needless words.
Vigorous writing is concise. A sentence should contain no unnecessary
words, a paragraph no unnecessary sentences, for the same reason that a
drawing should have no unnecessary lines and a machine no unnecessary
parts. This requires not that the writer make all his sentences short, or that
he avoid all detail and treat his subject only in outline, but that every word
tell.
In the 21st century, this advice still rings true as a concise (see, they’re doing
it!) definition of concise writing.
WHAT CONCISE WRITING IS
Concise writing is writing that has trimmed excess. It is writing that
practices the prescription to “omit needless words.”
The key phrase here is “needless words.” These are words that muddy your
writing: they distract the reader from central ideas, fail to carry their weight
in meaning or impact, or cloud the reader’s picture of the world you are
building. Even if you can’t point to these needless words, you’ll know they’re
there, because they make writing feel loopy, slack.
Again, concision is not about writing in a clipped or spare fashion. Concise
writing simply means writing that is clear, vivid, and impactful—writing in
which every word carries its weight. This is true whether you’re writing in
simple phrases or long lyrical sweeps.
And so the opposite of concise writing is not long writing; it is wasteful
writing, writing that fails to display an economy of language.
OMIT NEEDLESS WORDS: 11 ELEMENTS
OF CONCISE WRITING
Below are 11 writing habits that will tend to maximize the impact of each
word you write. Some are positive patterns to consider adopting. In other
cases, it’s easier to understand concise writing when you see what not to do.
Learning to identify common mistakes that lead to wasteful writing will help
you greatly with writing concisely.
1. USE CONCRETE LANGUAGE AND EXPRESSIVE
VERBS
The easiest way to write concisely is to use descriptive language. If you can
condense an idea into a single word, it often makes more sense to use that
word than to overstate it in 10 words. Expressive language goes hand-in-
hand with concision.
One of my favorite authors, Ruth Ozeki, has mastered expressive language.
Let’s look at the opening passage from her novel All Over Creation:
Concise example:
Active verbs are in orange, nouns are in aqua, and adjectives are in red.
This paragraph is filled with nouns and active verbs, which are the core of
expressive language. With a little bit of connective tissue (prepositions,
pronouns, articles, etc.), those nouns and verbs combine into a beautiful
image of our planet as a peach.
Notice, also, the frequency of each word category. Ozeki uses an equal
amount of concrete nouns and verbs; additionally, she occasionally uses
adjectives, though only about one-fourth as frequently as nouns and verbs.
This expressive language comprises half of the entire passage.
What would a wordy, inexpressive version of the same passage look like? I’ve
tried my best to recreate the verbose, unconcise alternative:
Wasteful example:
Active verbs are in orange, nouns are in aqua, and adjectives are in red.
In this example, there are far more of those connective words. Further, many
of the nouns and verbs are far from expressive: “story,” “corresponding,” and
“never-ending violence” are concepts without specific images, and the
reader isn’t clear on what the narrator is trying to compare.
When you’re struggling with a sentence, focus on the nouns and verbs; often,
you need very little else.
The gist: words can carry a lot of weight, so let them. When you’re struggling
with a sentence, focus on the nouns and verbs; often, you need very little
else.
2. WRITE IN THE ACTIVE VOICE
In our previous examples, we only highlighted the active verbs. It’s also
possible to write in the passive voice but doing so is generally less concise.
In an active voice sentence, the subject does the action (the verb). In a passive
voice sentence, the verb happens to the subject.
In active voice writing, the subject does the action. In passive voice
writing, the action happens to the subject.
Active voice: My wife visited the beach.
Active voice: The rain is drenching us.
Passive voice: The beach was visited by my wife.
Passive voice: We are being drenched by the rain.
Passive voice phrases always use the verb “to be” in some form.
• Be
• Are
• Is
• Was
• Been
• Were
• Being
Passive voice has a place in writing, but it often adds excess to your
sentences. The passive voice waters down your verbs, making them less
direct and impactful. Let’s take a quote from The Princess Bride and move it
into the passive voice:
Active: Hello. My name is Inigo Montoya. You killed my father.
Prepare to die.
Passive: Hello. I am named Inigo Montoya. My father was killed by
you. You are encouraged to prepare to die.
Hopefully this gives a sense of passive voice writing: it tends to feel wordy,
bureaucratic, impersonal. In active voice writing, the subject takes action. In
passive voice writing, action happens to a passive subject, and that makes for
alienating reading.
As a note, scientific and bureaucratic writing intentionally use the passive
voice: writers in these fields want to be speaking impersonally, and so they
change phrases like “The government requires your tax payment” to “Your
tax payment is required” or “The five of us identified a new protein” to “A
new protein was identified.” But this is rarely the goal in more creative and
expressive forms of writing.
Don’t swear off passive verbs entirely, just use them sparingly. When you do
use the passive voice, use it to offer important descriptive details.
For example, here’s a great use of the passive voice, from the US Declaration
of Independence:
Appropriate use of passive voice: We hold these truths to be self-evident, that
all men are created equal, that they are endowed by their Creator with
certain unalienable Rights, that among these are Life, Liberty and the pursuit
of Happiness.
The passive voice emphasizes “men” and “equal,” shifting the lens from the
Creator to Mankind himself. For its time (1776), this sentiment is rather
radical, as it actively called for representative democracy in a world largely
ruled by divinely chosen kings.
You can also use passive voice when something happens outside of the
subject’s control. For example, if your character gets hit by a large beach ball,
despite being in line in a coffee shop, then the passive voice makes perfect
sense.
Appropriate use of passive voice: As he turned from the cash register to scan
for a seat, he was struck by an enormous beach ball, perhaps eight feet in
diameter.
Here, passive voice properly highlights something happening to a passive
subject: being hit by an unexpected beach ball. But concise writing is rarely a
series of these kinds of accidents: the character above took other actions,
like “turned from the cash register,” that would read very badly in passive
voice (“As the cash register was turned away from by him…”).
The bottom line: be careful about the passive voice, as it often creates
needless words.
3. WATCH FOR NEEDLESS REPETITION
A surprising amount of writing repeats itself. Redundancies occur inevitably
in writing but learning to recognize and condense them is a necessary
element of concise writing.
Redundant language is language that doesn’t provide new or unique
information. For example, if I started writing about the green grass, that
would be rather redundant, since grass—unless otherwise specified—is
green. If this greenness is somehow unusual, then perhaps it makes sense to
write about, but if the green grass is just a background part of the world I’m
building, I don’t need to tell you it’s green—and you don’t need to be told,
either.
We also let redundancies slip when relying on colloquial turns of phrase. A
lot of the phrases we use in English are redundant, especially transition
statements and verbal colloquialisms. To give you an example, the chart
below lists common English phrases on the left, with concise synonyms on
the right.
After seeing these examples, you may notice how easily redundancies slip
past us. We speak many of these phrases without thinking about them; but as
writers, we have to start noticing when our words aren’t working. Limiting
your use of adjectives, adverbs, and colloquial phrases is a good place to
start: each is a hotbed of redundancy, as we’ll discuss.
4. LIMIT YOUR USE OF ADVERBS
I’ve just mentioned a crucial rule for concise writing: limit your adverbs and
adjectives. What do I have against these parts of speech? It’s nothing
personal, but when writers omit needless words, adverbs are often the first
to go.
What are adverbs? They are words that modify verbs or adjectives. Adverbs
act like adjectives for verbs (and sometimes for adjectives), offering
additional information about the way the verbs are performed.
Let’s look at a simple sentence that uses an adverb:
Sentence with adverb: The dog sat lazily on the couch.
Here, the adverb “lazily” modifies the verb “sits.” As the author of this
sentence, I have decided that the dog’s laziness is crucial information for the
reader.
However, I have also used an extra word to describe that laziness. The
English language has a lot of words: we often don’t need to use an adverb
because there’s a verb that denotes the same thing. The following adverb-
less sentences all convey the same information, in fewer words:
• The dog rested on the couch.
• The dog relaxed on the couch.
• The dog lounged on the couch.
• The dog lazed on the couch.
Each of these examples portrays different aspects of laziness, without ever
using the word “lazily.”
Adverbs often violate the “Show, don’t tell” rule of writing, as they present
excess information instead of inviting the reader to visualize the scene
directly. See the following two examples for “Show, don’t tell” alternatives to
our initial sentence:
• The dog curled into the couch.
• The dog sighed and sank deeper into the couch cushions.
The issue is not word count; it’s that “lazily” works—well, lazily. It tells your
readers what to know rather than inviting them into the world you’re
creating, or at least looking for a verb that could carry its weight. It’s a tiny,
missed opportunity.
Don’t forswear adverbs entirely, in part because “entirely” is an adverb. But
the thesaurus is often your friend as a writer, and if a shorter equivalent
exists, you should use it most of the time. If you can’t find the right verb,
then go ahead and use an adverb: I don’t think there’s a simple English
synonym for an activity like “surf ironically.” But for phrases like “destroy
completely” (“obliterate,” “annihilate,” or simply “destroy”) or “descend
suddenly” (“plummet,” “tumble,” “fall”) you’ve definitely got some options.
5. DON’T OVERUSE ADJECTIVES
Overuse of adjectives can also threaten concise writing. You don’t want an
adjective to do a noun’s work; if there’s a noun that’s easy to visualize, you
don’t need adjectives to modify that noun.
For example, you don’t need to tell us about the color of a fire hydrant—
unless that color is notably different.
Unnecessary: They drove past the red fire hydrant.
Most readers, at least in the United States, will assume that the fire hydrant
is red.
Better: They drove past the fire hydrant.
Or, necessary again: They drove past the aquamarine fire hydrant.
What an unusual color for a fire hydrant! Perhaps this is a doorway to other,
equally discolored objects throughout the city. Perhaps the road paint is
green, the stop signs are indigo, and the streetlights shine like spider’s silk in
moonlight.
Of course, this doesn’t apply just to colors. Adjectives can be used in any of
the following ways:
• Physical traits: furry, soft, lukewarm, etc.
• Emotional traits: happy, excited, suspicious, etc.
• Quantities: three flowers, six moons, two roommates, etc.
• Comparison: the tastier drink, the ugliest house, the hottest day, etc.
Overuse of adjectives leads to a muddy, overemphatic style that, in extreme
cases, feels like the writer is having readers’ experience for them:
Muddy: We savored the cool creamy white-and-red strawberry milkshakes,
an almost sinfully delicious relief on a sweltering midsummer July day.
Consider which adjectives really matter for the reader’s own experience:
Better: We enjoyed strawberry milkshakes in the July heat.
Let your nouns do most of the work and bring in the occasional adjective to
help paint a more vivid picture.
Writers need adjectives more frequently than they need adverbs, but don’t
overuse them: hopefully, fewer than half of your nouns will carry adjectival
modifiers. Let your nouns do most of the work and bring in the occasional
adjective to help paint a more vivid picture.
6. ONE IDEA PER SENTENCE
In ye olde days of classic literature, writers often wrote sprawling sentences
that covered a wide range of images or ideas without pause. Just take a look
at the opening sentence of A Tale of Two Cities:
It was the best of times, it was the worst of times, it was the age of wisdom, it
was the age of foolishness, it was the epoch of belief, it was the epoch of
incredulity, it was the season of Light, it was the season of Darkness, it was
the spring of hope, it was the winter of despair, we had everything before us,
we had nothing before us, we were all going direct to Heaven, we were all
going direct the other way—in short, the period was so far like the present
period, that some of its noisiest authorities insisted on its being received, for
good or for evil, in the superlative degree of comparison only.
These are classic lines in Western literature, and they work partly because
their chaos and jumble nicely mirror the chaos and jumble of the French
Revolution. However, we strongly encourage you not to write this way unless
you’ve got an extremely good reason. To juxtapose numerous big concepts in
a breathless jumble worked for Dickens in this instance, but it’s much less
likely to work for modern sensibilities, and even here it makes for confusing
and almost overwhelming reading.
The key word is “breathless”: in prose like the above there’s no chance for us,
as readers, to collect our thoughts. This is where “one idea per sentence”
comes in as good advice.
Let’s take a sentence that says too many things and see how we might
rework it.
Confusing: The car’s totaled; you’ll never fix it, and even if you could I don’t
know how that helps me find work since no one’s hiring, not since the
bauxite mine closed.
Easier to read, but now choppy: The car’s totaled. You’ll never fix it. Even if
you could, I don’t know how that helps me find work. No one’s hiring, not
since the bauxite mine closed.
Clearer: The car’s totaled; you’ll never fix it. Even if you could, how does that
help me find work? No one’s hiring, not since the bauxite mine closed.
The reworked example above takes care not to chain together choppy
declarative sentences—which can be one risk of overapplying a “one idea per
sentence” style—and manages to convey each core idea with more clarity and
focus.
7. AVOID QUALIFYING SENTENCES
Qualifying sentences exist to modify another sentence, adding contributing
details. They often end up adding needless words.
Wordy: Clifford is a dog. He is big and red.
Concise: Clifford is a big red dog.
The wordy example above has a second sentence tacked awkwardly after the
first sentence, providing information that the first sentence could provide.
In general, try to limit your use of qualifying sentences. Instead, combine
descriptions into single sentences, and omit needless words from there.
8. DON’T OVERRELY ON AUXILIARY WORDS
Auxiliary words are “connective tissue” words. They don’t tell you the main
information, but they help provide direction, clarifying the relationship
between nouns and verbs. Most sentences require a little bit of connective
tissue, but only write auxiliary words when needed; there are many
opportunities for you to omit needless words here.
The following are types of auxiliary words:
• Auxiliary verbs: these are verbs which indicate action while also
modifying the main verb of a sentence. In English, the main auxiliary
verbs are be, do, and have. In the sentence “I can help you,” “can” is the
auxiliary verb because it modifies “help,” the main verb. (“Help” is the
main verb because it is the main action. “I can you” isn’t a sentence, but
“I help you” is.)
• Prepositions: words that provide direction for verbs. These words
indicate how a verb affects a noun. Some prepositions include: for,
from, to, in, out, on, off, among, with, without, across, about, above,
below, since, under, and through.
• Conjunctions: words that connect or separate nouns. The three
conjunctions are “and,” “but,” and “or.”
• Determiners: These are words which exist solely to indicate
something. Definite determiners include words like “that,” “these,”
“who,” and “which.” (“A,” “an,” and “the” are articles, which is the
“indirect” form of a determiner. Articles are grammatically necessary;
non-articles are often optional.)
• Pronouns: these are words that stand in place for nouns. You should
only use pronouns if the noun has already been used, and you should
use them primarily for stylistic purposes—to avoid awkward repetitions
or to write dialogue, for example. Some pronouns include: he, his, her,
hers, they, theirs, who, your, and it.
Auxiliary word bonanza: My dragon has a tail that is very long and that
stretches above my roof, and which could probably touch a passing jet if it
was following a flight path that was passing low to the ground somewhere
nearby us.
Better: My dragon’s long tail stretches above my roof and could probably
touch a passing jet with a low flight plan.
Try to write sentences that are mostly nouns, verbs, and adjectives, and use
auxiliary words when they are grammatically necessary or present crucial
information.
9. LIMIT TURNS OF PHRASE
The English language has countless idioms, colloquial expressions, and
vernaculars. In other words, turns of phrase—a great opportunity to omit
needless words.
A turn of phrase is any sort of expression that the audience won’t understand
from context alone. The phrase means something that the words themselves
don’t denote, such as the phrase “under the weather,” which means “sick.”
Many figures of speech are both wordy and clichéd. Occasionally, you can
work a figure of speech into something tongue-in-cheek, but often, using
them in your writing simply substitutes others’ generalities for your own
specifics.
Clichéd: When he blew off our date, it only added fuel to the fire.
Better: I grew even angrier at him when he didn’t show up for our date.
Here’s a great list of common idioms to avoid for most English dialects. Turns
of phrase simply don’t carry the same weight that concise writing does, so let
your words speak for themselves.
10. LIMIT THE USE OF FANCY WORDS
In general, this article advises using fewer words where possible, but this is
certainly not an absolute rule. Simple multi-word phrases are often
significantly clearer than obscure, esoteric words that say the same thing.
Simple writing is more concise—in the sense that the reader can more easily
glean meaning—even if it uses more words overall.
Too fancy: Clandestinely, Jordan lamented being the school’s cynosure.
Readable: Secretly, Jordan regretted being the school’s center of attention.
Although the “smart-sounding” sentence is concise, it’s far from readable.
And longer words often carry connotations you may not want:
“clandestinely” isn’t actually a great word choice in the first sentence above,
because it usually means “secretive in the manner of professional spies”
rather than “secretive in the manner of shy high school students.”
Concision means maximizing what we offer readers for their time and effort.
Big words sometimes help with this: if we write “a clandestine meeting”
rather than “a meeting conducted in an environment of secrecy,” we’ve
helped our readers. But a habit of using big words runs counter to concision.
Use “fancy” words sparingly when they illuminate something that simpler
language cannot.
11. AVOID OVERSTATEMENT
Overstatement is any sort of excessive, superlative description. It’s not just a
red car, it’s the reddest car on the road; it’s not just a warm day, but the hottest
day imaginable.
Overstatement aims at maximum impact—but it actually reduces impact, by
diminishing the writer’s legitimacy. It’s like someone who bangs the table
after everything he says: can we take this person seriously? Is
everything that emphatic?
Overwritten: My passion was all-consuming, surpassing, mind-numbing, a
raging bonfire that engulfed the entire universe and collapsed all of reality
into a single infinitely beguiling point: him.
Better: I felt overwhelming passion for him.
Hyperbole can be an effective literary device. However, you should use
superlative descriptions sparingly. The imagery in your writing should do the
work for the reader, showing them the red car or hot day rather
than telling them that it’s a huge deal.
ARE THESE CONCISE WRITING TIPS
UNIVERSAL?
In other words, should you abide by all of these tips, all the time? Not at
all. Writers break the rules all the time, testing the barriers of language and
meaning. However, in order for us to break the rules, we must first learn the
rules.
And our rule-breaking should be within an overall goal of best using the
reader’s time and attention—which is what concise writing and “omit
needless words” is all about. That’s not a rule worth breaking.

#### ChatGPT:
<time datetime="2025-02-14T15:30:05.589Z" title="2/15/2025, 12:30:05 AM">00:30</time>

Below is a revised, more concise version of the argument against “the positive benefits of AI are greater than the negative tradeoffs for society.” I have aimed to “omit needless words,” following principles of clarity and brevity outlined in the style guide.

---

Artificial intelligence (AI) is often hailed for revolutionizing healthcare, transportation, and education. Advocates emphasize productivity gains, energy savings, and data-driven decision-making. Yet, these arguments frequently overlook the serious negative tradeoffs: AI’s soaring energy and water demands, abuses of personal data, manipulative user design, entrenchment of biases, corporate power imbalances, and inadequate governance. These problems suggest that AI’s adverse impacts may eclipse its touted benefits.

## 1. Environmental Burdens: Energy and Water

Despite claims that AI can help the environment, recent evidence highlights its large, and increasing, resource footprint. Data centers worldwide use roughly **460 terawatt-hours (TWh)** of electricity every year—a figure that accounted for about **2% of global electricity consumption** in 2022. AI workloads alone consumed **5–9 TWh** in 2023, with forecasts suggesting a steep climb to hundreds of TWh by the decade’s end. The resulting emissions and power demands strain grids and may offset efficiency gains AI supposedly provides in other sectors.

Energy alone doesn’t capture the full picture. AI data centers require millions of gallons of water each day to prevent overheating. A single large facility can draw **1 to 5 million gallons daily**, enough to supply entire municipalities. Microsoft’s water consumption soared **34%** from 2021 to 2022, reaching 1.7 billion gallons a year, largely due to AI growth. In deserts like Arizona, local reservoirs dwindle as massive data centers compete with everyday users for freshwater. Uruguay witnessed protests over Google’s plans for a data center with comparable water demands, aggravated by drought. Even if AI yields some energy optimizations, its own thirst for electricity and water can reduce or negate these gains.

## 2. Materials Extraction and E-Waste

AI hardware requires raw minerals—**cobalt**, **lithium**, and **rare earth elements**—mined under poor labor and ecological conditions. Cobalt extraction in the Democratic Republic of Congo frequently involves child labor. Lithium processing in Chile’s arid Atacama region depletes water sources vital to local communities. Manufacturing advanced semiconductors also causes pollution, as it relies on hazardous chemicals and large carbon outputs.

Once deployed, AI servers undergo rapid refresh cycles to maintain cutting-edge performance. Companies often replace chips, GPUs, and entire racks every few years. This turnover produces e-waste, much of which ends up in landfills or is shipped to nations lacking safe recycling infrastructure. In 2022, an estimated 62 million metric tons of electronic waste were generated globally—much unprocessed, leaking toxins into soil and water.

## 3. Privacy Violations and Data Misuse

Powerful AI models rely on massive amounts of user data—often collected without explicit user understanding. ChatGPT and Bard log conversation prompts, timestamps, and other personal details. Voice assistants like Amazon Alexa and Google Assistant gather audio recordings and transcripts. These logs feed corporate AI training pipelines, raising concerns about consent and data exploitation.

Italy temporarily banned ChatGPT in 2023 for **General Data Protection Regulation (GDPR)** violations, citing unauthorized data collection and insufficient age checks. While AI proponents celebrate “personalization,” the hidden cost is the erosion of personal control over one’s information. Centralizing data in the hands of a few AI giants creates tempting targets for hackers and can facilitate invasive surveillance—by corporations or governments.

## 4. Manipulative Design, Addiction, and Mental Health

AI also fuels engagement-driven platforms that hijack attention. Algorithms on YouTube, TikTok, and Facebook feed users an endless loop of videos and posts. Research on social media usage ties compulsive scrolling to depression, anxiety, and diminished attention spans. Automated recommendations rely on reward systems—likes, shares, notifications—that exploit psychological vulnerabilities and FOMO (fear of missing out).

Proponents say these platforms “connect communities” or “foster creativity,” yet the constant pursuit of higher engagement has real mental health effects. The University of Pennsylvania found that limiting social media to \~30 minutes a day reduced loneliness and depression. AI’s ability to tailor content can turn it into a tool for reinforcing harmful usage patterns, not just delivering entertainment.

## 5. Bias, Underrepresentation, and Cultural Erosion

Most AI systems are trained on English-language or Western-centric datasets, embedding biases that disadvantage regions lacking digital infrastructure. Less-common dialects and local knowledge remain underrepresented, resulting in models that fail to understand crucial nuances or perpetuate stereotypes about non-Western communities. Studies from Penn State to the University of Washington reveal that generative AI often distorts or erases non-Western cultural norms.

Even well-intentioned AI expansions rarely include wide linguistic or cultural coverage. Large companies focus on profitable markets first, leaving marginalized users dependent on inadequate or inaccurate systems. While open-source initiatives like BigScience’s BLOOM attempt more inclusive data sets, these remain exceptions. The net effect is a reinforcement of global cultural asymmetries.

## 6. Corporate Control and Monopolization

AI’s dominance by a handful of companies—Google, Microsoft/OpenAI, Meta, Amazon—limits both public input and market competition. The UK government reported in 2023 that these top firms capture about **40%** of all AI-related revenue. Smaller competitors and academic labs lack the billions of dollars and proprietary data pipelines required to train equally large models. Thus, a small circle of corporations decides which features to prioritize or which user communities to serve.

Moreover, hardware supply chains revolve around a near-monopoly in AI GPU manufacturing, with NVIDIA controlling about **92%** of that market. This concentration grants a few players near-total authority over model creation and deployment. Their internal priorities set the global course for AI, often sacrificing transparency. GPT-4, for example, debuted with minimal technical disclosures, reducing accountability for issues like potential bias or security flaws. This centralization also invites complacency: each corporate release drives usage, overshadowing smaller attempts to build local solutions or address niche needs.

## 7. Weak Regulation and Governance

Advocates point to new regulations, such as the EU’s proposed AI Act or China’s detailed generative AI measures, as proof that negative impacts will be curbed. Yet these frameworks remain nascent. The EU AI Act adopts a risk-based approach but must still tackle how to enforce transparency from powerful, secretive corporations. Meanwhile, the U.S. has only rolled out voluntary guidelines like the White House’s “Blueprint for an AI Bill of Rights.” Many issues—labor displacement, e-waste, data ownership—remain unaddressed by current laws.

China’s regulatory approach underscores another risk: a government can centralize AI for mass surveillance. Western democracies might not apply the same censorship rules, but facial recognition or predictive policing tools have proliferated in the West, often without robust legislative debate. If no global standard sets strong ethical guardrails, big AI vendors can enact proprietary rules that might ignore local communities or pressure governments into adopting suboptimal solutions.

## 8. Anticipating Pro-AI Arguments—and Why They Fall Short

**Claim**: “AI will rescue the climate via system-wide efficiency.”
**Counter**: Training large language models demands enough electricity to power entire cities. Companies also withdraw enormous volumes of water for cooling. The net effect can undercut prospective energy savings from better logistics or climate modeling.

**Claim**: “AI fosters job creation and boosts economic growth.”
**Counter**: While new technical roles emerge, AI also threatens widespread automation. Many routine jobs risk elimination, and it’s unclear if those job losses will be offset. Even if AI lifts overall GDP, the distribution of those gains typically favors corporations with the resources to deploy it at scale.

**Claim**: “Personalization improves daily life and supports education.”
**Counter**: This personalization relies on extensive data collection. Users pay with their privacy while ceding control over how content is selected or moderated. Students who depend on AI-based learning tools can be exposed to subtle biases, incomplete curricula, and unverified data.

**Claim**: “Regulations will ensure responsible AI use.”
**Counter**: Existing regulation targets narrow areas—like generative outputs or data protection—yet struggles to address resource consumption, manipulative design, or the unstoppable growth of corporate power. Effective oversight still lags behind the dizzying pace of AI releases.

## 9. Conclusion: The Tradeoffs Are Substantial

Though AI brings recognized advantages, its negative tradeoffs—growing resource demands, relentless data capture, user manipulation, bias, and runaway centralization—are massive and immediate. Corporations commanding AI set the terms for everyone, while ecological and societal costs accumulate. Real reform would require open-source models, strict environmental standards, privacy protections, and transparent governance.

Until such comprehensive changes occur, the notion that AI’s benefits outweigh its costs is unconvincing. The real gains often serve those controlling the technology, while large segments of society bear the drawbacks: water shortages, data exploitation, and limited recourse against bias or manipulation. AI’s promise of a better tomorrow rings hollow if it fails to address the harm it exacts on human communities, local ecosystems, and individual rights.

Far from presenting unalloyed benefit, AI in its current state imposes heavy burdens—burdens that appear to exceed its widely promoted gains.


#### You:
<time datetime="2025-02-14T15:35:48.280Z" title="2/15/2025, 12:35:48 AM">00:35</time>

finally, add citations to the claims made in this position. use the research from above and add additional research if neded.

#### ChatGPT:
<time datetime="2025-02-14T15:36:04.971Z" title="2/15/2025, 12:36:04 AM">00:36</time>

To properly cite claims in the position paper, I need to gather credible sources on the following:

1. AI’s energy consumption and projected growth.
2. AI’s water consumption and impact on local resources.
3. AI’s role in e-waste and material extraction.
4. Privacy concerns and data exploitation by AI-driven platforms.
5. The psychological impact of AI-driven social media algorithms.
6. AI biases and underrepresentation in global datasets.
7. AI’s monopolization and control by large corporations.
8. Current regulatory efforts and their limitations.
9. Economic impact, including job losses and job creation arguments.

Could you confirm if you’d like sources from peer-reviewed journals, government reports, industry reports, or a mix of all three? Also, should the citations follow a specific format (APA, MLA, Chicago)?


#### You:
<time datetime="2025-02-14T15:36:55.920Z" title="2/15/2025, 12:36:55 AM">00:36</time>

peer-reviewed journals, government reports, other highly credible sources that likely have less bias than privately-funded industry  reports.

use whichever style is simplest

#### ChatGPT:
<time datetime="2025-02-14T15:37:09.954Z" title="2/15/2025, 12:37:09 AM">00:37</time>

I will gather peer-reviewed journal articles, government reports, and other highly credible sources to support claims related to AI's environmental, social, and economic impacts. This will include sources on AI’s energy and water consumption, e-waste contribution, privacy risks, psychological effects, biases, monopolization concerns, regulatory challenges, and economic implications.

Once I have compiled these sources, I will format citations in a simple, clear style for easy integration into your position paper.


#### ChatGPT:
<time datetime="2025-02-14T15:44:02.333Z" title="2/15/2025, 12:44:02 AM">00:44</time>

# Key Issues in AI: Evidence and Sources

## AI’s Energy Consumption and Projected Growth in Energy Demand

AI development is **energy-intensive**, heavily relying on power-hungry data centers. Globally, data centers already account for roughly *0.5%* of all electricity use【59†L427-L434】. As AI workloads grow, studies project a sharp rise in energy demand: for example, worldwide AI-related electricity consumption could increase by **85–134 TWh per year by 2027**【57†L69-L77】 – an amount comparable to a small country’s annual usage. In the U.S., analysts estimate AI data centers may require an extra **14 gigawatts** of power capacity by 2030 to meet demand【59†L427-L434】, underscoring concerns about infrastructure strain and higher carbon emissions.

## AI’s Water Consumption and Impact on Local Water Resources

Running AI models at scale also **consumes vast quantities of water** for cooling server farms. A recent peer-reviewed analysis estimates that **global AI computing could withdraw 4.2–6.6 billion cubic meters of water in 2027** – roughly half of the United Kingdom’s total annual water usage【14†L19-L27】. Such thirst can have **local impacts**: individual AI data centers can drain municipal water supplies. For instance, a planned Microsoft AI campus in arid Arizona is projected to use about **56 million gallons of drinking water each year**, equivalent to the yearly use of \~670 families【12†L11-L16】. These examples highlight how AI’s water demands can exacerbate resource stress in drought-prone communities.

## AI’s Contribution to E-Waste and Material Extraction

The AI boom carries a **hidden environmental cost** in hardware production and disposal. Manufacturing advanced electronics is resource-intensive – for example, producing a single 2 kg computer requires around **800 kg of raw materials**【60†L1-L4】. The microchips powering AI depend on **rare earth elements**, which are often mined in environmentally destructive ways【60†L1-L4】. Moreover, the rapid turnover of servers leads to mounting **electronic waste (e-waste)**. Data centers eventually discard large volumes of hardware that contain hazardous substances like mercury and lead【61†L1-L4】. This e-waste contributes to pollution and adds pressure on recycling and waste management systems, linking AI to broader sustainability challenges in mining and waste disposal【16†L175-L183】【16†L219-L227】.

## Privacy Concerns and Data Exploitation by AI Platforms

**Privacy issues** are a major concern as AI-driven platforms collect and monetize personal data at an unprecedented scale. AI systems thrive on big data; indeed, experts warn that today’s largely unrestrained data harvesting poses *“unique risks to privacy”* at a societal level, beyond individual control【62†L1-L4】. There have been real-world cases of data exploitation: for example, a major online ad tech firm (Criteo) was **fined €40 million in France for unlawfully collecting personal data** for targeted advertising【64†L111-L119】. Such cases illustrate how AI platforms can abuse user information, prompting regulators to scrutinize how algorithms gather, use, and potentially misuse personal data. The concern is that without stronger oversight, AI’s appetite for data could continue to erode consumer privacy and data rights【62†L1-L4】.

## Psychological Impact of AI-Driven Social Media Algorithms

AI-driven algorithms on social media are linked to **negative psychological effects**. Researchers and clinicians have observed rising problems like loneliness, anxiety, *“fear of missing out”*, and depression that correlate with heavy use of algorithm-curated social feeds【27†L77-L85】. These platforms employ AI to maximize engagement – often by showing content that triggers emotional responses – which can lead users into unhealthy usage patterns. Notably, studies have found that social media algorithms may amplify harmful content to vulnerable groups. One investigation showed that TikTok’s algorithm flooded teen users with extreme weight-loss videos, a practice **linked to increases in poor body image, eating disorders, and even suicidal ideation among adolescents**【29†L313-L321】. Such findings underscore the mental health risks posed by AI-tuned recommendation systems and have spurred calls for more “humane” algorithm designs or regulatory interventions to protect users (especially youth)【29†L313-L321】.

## AI Biases and Underrepresentation in Global Datasets

AI systems often exhibit **biases** when trained on unbalanced datasets, leading to unfair outcomes for underrepresented groups. A landmark study on commercial face recognition found stark performance disparities: the error rate was as low as **0.8%** for light-skinned men but jumped to **34.7%** for dark-skinned women【37†L121-L128】. This gap, caused by skewed training data, highlights how AI can inadvertently perpetuate gender and racial biases. On a global scale, there is also **uneven representation in AI datasets and research**. A recent review of hundreds of medical AI studies showed that U.S. and Chinese data (and authors) are *“disproportionately overrepresented”* in clinical AI research, with most of the top 10 datasets coming from high-income countries【65†L388-L397】. Populations from less wealthy regions are often **underrepresented**, raising concerns that AI models may not generalize well worldwide and could entrench existing disparities. These findings reinforce the need for more diverse data collection and bias mitigation in AI development【37†L123-L128】【65†L388-L397】.

## AI’s Monopolization and Control by Large Corporations

The **power in AI development is increasingly concentrated** in the hands of a few large tech companies. Modern AI research requires enormous datasets, expert talent, and expensive computing infrastructure – resources that *“are often only available to a small group of for-profit firms.”* Consequently, analysts warn that *“only a handful of companies”* will be able to perform cutting-edge AI work, leading to a *“worrisome concentration of power in AI development”*【40†L1-L4】. Evidence of this trend can be seen in workforce data: roughly **70% of new AI PhD graduates now join industry** (private companies), a dramatic rise from about 20% two decades ago【43†L193-L201】. Big Tech firms not only control key AI research and computing platforms, but also increasingly set research agendas and standards. This monopolization of AI capabilities by corporate giants has raised concerns about reduced competition, less transparency, and potential misuse of AI, fueling debates about antitrust measures and the need for open, public-interest AI research【40†L1-L4】【43†L193-L201】.

## Current Regulatory Efforts on AI and Their Limitations

Governments have begun to propose **AI regulations**, but current efforts have significant limitations. In the United States, **no comprehensive federal AI law** exists as of yet – oversight is instead patchwork, relying on existing laws and a growing number of state initiatives【69†L1-L4】. Over 100 AI-related bills are under consideration in Congress, but most focus on voluntary guidelines, and few have passed, reflecting a cautious approach that many say leaves regulatory gaps【47†L596-L604】. The European Union is moving forward with its landmark **AI Act**, a broad risk-based regulatory framework. However, experts point out that intense lobbying by big tech and EU member states has **“watered down” the AI Act**, resulting in an overreliance on **self-regulation and self-certification**, relatively weak oversight mechanisms, and broad exceptions for both public and private sector AI systems【70†L1-L4】. These loopholes mean that many AI applications might evade strict scrutiny. In short, while the EU and some other jurisdictions are pioneering AI-specific rules, enforcement challenges and carve-outs remain, and in much of the world AI governance is still in its infancy – often lagging behind the technology’s rapid advancement【70†L1-L4】【69†L1-L4】.

## Economic Impact of AI: Job Losses vs. Job Creation

AI’s impact on employment is **double-edged**, with both job displacement and job creation effects. On one side, AI and automation are expected to **eliminate or transform certain jobs**, especially roles involving routine, repetitive tasks. For instance, the World Economic Forum’s analysis finds that many administrative and clerical positions are likely to **“decline quickly because of AI,”** as intelligent systems take over basic office duties and data processing【71†L31-L39】. Indeed, forecasts by McKinsey and others predict tens of millions of jobs worldwide could be automated in the coming decade. However, it’s not all loss. AI is also **creating new jobs and demand for new skills**. The same WEF report notes growing need for **AI and machine-learning specialists, data analysts, and digital transformation experts** – roles expected to grow rapidly as AI is adopted【71†L31-L39】. History suggests that technology-driven productivity boosts often lead to the emergence of new industries and occupations. Economists observe that past *“technological unemployment”* fears have repeatedly proven false, as **macroeconomic job creation** in new sectors eventually offset the **micro-level disruptions**【55†L33-L40】. In the long run, AI is projected to increase productivity and generate wealth, which can create entirely new categories of employment. The consensus of many labor experts is that **AI will alter the nature of work rather than simply causing mass unemployment** – the challenge is managing the transition, retraining workers, and ensuring the gains are broadly shared【55†L33-L40】.
